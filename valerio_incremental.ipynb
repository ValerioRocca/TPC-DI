{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From WatchHistory.txt to FactWatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    working_df = []\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    elif i[1] == \"FactWatches\":\n",
    "        # SK_CUSTOMERID and SK_SECURITYID: slighly different (added \"IsCurrent=1\").\n",
    "        # Put in as a comment as this is how it was done in FactWatches's historical load\n",
    "        SK_CUSTOMERID = 0 #spark.sql(\"select SK_CustomerID from DimCustomer where CUSTOMERID = '\" + row[0] + \"'AND IsCurrent = 1\")\n",
    "        SK_SECURITYID = 0 #spark.sql(\"select SK_SecurityID from DimSecurity where SYMBOL = '\" + row[1] + \"'AND IsCurrent = 1\")\n",
    "        SK_DATEID_DATEPLACED=row[2]\n",
    "        if row[3]==\"ACTV\":\n",
    "            SK_DATEID_DATEREMOVED = ''\n",
    "        else:\n",
    "            SK_DATEID_DATEREMOVED = row[2]\n",
    "        BATCHID=1\n",
    "\n",
    "        working_df = [SK_CUSTOMERID,SK_SECURITYID,SK_DATEID_DATEPLACED,SK_DATEID_DATEREMOVED,BATCHID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Prospect file to Prospect table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did I perform the update?\n",
    "1. Insert all the values in another table (e.g. \"temp_Prospect\")\n",
    "2. When finished, check if there are values of \"Prospect\" not in \"temp_Prospect\". In case, insert them into \"temp_Prospect\". If you think about it, the output is exactly the same of the update.\n",
    "3. Truncate table \"Prospect\"\n",
    "4. Create table \"Prospect\" and copy the values from \"temp_Prospect\"\n",
    "5. Truncate table \"temp_Prospect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just after calling your .sql file to create the table, run this line to\n",
    "# create the temporary table\n",
    "spark.sql(\"CREATE TABLE temp_Prospect AS SELECT * FROM Prospect\")\n",
    "\n",
    "#...\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    working_df = []\n",
    "    \n",
    "    #...\n",
    "    \n",
    "    elif i[1] == \"Prospect\":\n",
    "             \n",
    "        isCustomer = spark.sql(\"select * from DimCustomer where status = 'ACTIVE' \")#to-do\n",
    "        if isCustomer.count() == 0:\n",
    "            isCustomer = 0\n",
    "        else:\n",
    "            isCustomer = 1\n",
    "        SK_RecordDateID = 0 #to-do (both historical and incremental)\n",
    "        SK_UpdateDateID = 0 #to-do (both historical and incremental)\n",
    "        \n",
    "        MarketingNameplate = \"\"\n",
    "        if row[12] > 200000 or row[21] > 1000000:\n",
    "            MarketingNameplate += \"+HighValue\"\n",
    "        if row[14] > 3 or row[20] > 5:\n",
    "            MarketingNameplate += \"+Expenses\"\n",
    "        if row[16] > 45:\n",
    "            MarketingNameplate += \"+Boomer\"\n",
    "        if row[12] < 100000 or row[21] < 50000 or row[17] < 600:\n",
    "            MarketingNameplate += \"+MoneyAlert\"\n",
    "        if row[13] > 3 or row[20] > 7 :\n",
    "            MarketingNameplate += \"+Spender\"\n",
    "        if row[16] < 25 and row[12] > 1000000 :\n",
    "            MarketingNameplate += \"+Inherited\"\n",
    "        if len(MarketingNameplate) != 0:\n",
    "            MarketingNameplate = MarketingNameplate[1:]\n",
    "        \n",
    "        \n",
    "        working_df = [row[0],SK_RecordDateID,SK_UpdateDateID,1,isCustomer\n",
    "                    ,row[1],row[2],row[3],row[4],row[5],row[6]\n",
    "                    ,row[7],row[8],row[9],row[10]\n",
    "                    ,row[11],row[12],row[13],row[14],row[15],row[16],\n",
    "                    row[17],row[18],row[19],row[20],row[21],MarketingNameplate]\n",
    "        print(MarketingNameplate)\n",
    "        \n",
    "        # ...\n",
    "\n",
    "    csv_writer.writerow(working_df)\n",
    "\n",
    "#add result \n",
    "sch = spark.sql(\"select * from \" + i[1]).schema\n",
    "df = spark.read.schema(sch).csv(os.getcwd() + \"/result/temp_\" + i[1] + \".csv\", sep=',')\n",
    "if i[1] != \"Prospect\"\n",
    "    #df.write.mode(\"append\").insertInto(i[1])\n",
    "else:\n",
    "    # Insert batch2's Prospect table values into the temporary table\n",
    "    df.write.mode(\"append\").insertInto(\"temp_Prospect\")\n",
    "    # Extract batch1's values not present in batch2\n",
    "    input_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM Prospect as p\n",
    "    WHERE p.AgencyID NOT IN\n",
    "    (\n",
    "        SELECT tp.AgencyID\n",
    "        FROM temp_Prospect as tp\n",
    "    );\n",
    "    \"\"\"\n",
    "    # Append them to the temporary table, which now contains \"the updated\"\n",
    "    # values and the non-modified ones from Batch1\n",
    "    spark.sql(input_query).write.insertInto(\"temp_Prospect\", overwrite=False)\n",
    "    # Erase all Prospect rows and copy temp_Prospect into it\n",
    "    spark.sql(\"TRUNCATE TABLE Prospect\")\n",
    "    spark.sql(\"INSERT INTO Prospect SELECT * FROM temp_Prospect\")\n",
    "    # Erase all temp_Prospect rows\n",
    "    spark.sql(\"TRUNCATE TABLE temp_Prospect\")\n",
    "    \n",
    "    #...\n",
    "    \n",
    "    # At the start of the audit phase, erase \"temp_Prospect\"\n",
    "    spark.sql(\"DROP TABLE IF EXISTS temp_Prospect\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
