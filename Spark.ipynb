{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:23:26 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.178.59 instead (on interface eno1)\n",
      "23/12/18 15:23:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/coucou/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/coucou/.ivy2/cache\n",
      "The jars for the packages stored in: /home/coucou/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-db426fcd-eb06-492d-9cfb-d445966229c9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.13.0 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;2.3.4 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.2.5 in central\n",
      ":: resolution report :: resolve 386ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.13.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.2.5 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;2.3.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-db426fcd-eb06-492d-9cfb-d445966229c9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/17ms)\n",
      "23/12/18 15:23:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# AUDIT VALERIO\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# path to files\n",
    "import glob\n",
    "\n",
    "#librairies to read/tranforme/write draw data\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# DIMESSAGES VALERIO\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"TPC-DS Data Loading\").config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.13.0\").config(\"spark.sql.catalogImplementation\", \"hive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags Valerio modification\n",
    "1. \"PERFORM UPDATE VALERIO\" -> Update of Prospect table in Incremental Phases\n",
    "2. \"AUDIT VALERIO\" -> Steps linked to perform the Audit Phase\n",
    "3. \"DIMESSAGES VALERIO\" -> Steps linked to output DImessages"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This part set up database with differents tables. It load data(the scale of data have to do manually and place into data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a set of variables\n",
    "# SETUP the name of the file where result will be write\n",
    "SF = 10 #choose between 1,3,5,8,10\n",
    "\n",
    "#order of table to load\n",
    "\n",
    "Not_modified_table = [[\"Time\", \"DimTime\"], [\"Date\", \"DimDate\"],[\"Industry\", \"Industry\"],\n",
    "                     [\"StatusType\", \"StatusType\"],[\"TaxRate\", \"TaxRate\"],[\"TradeType\",\"TradeType\"]]\n",
    "\n",
    "FINWIRE_table = [[\"FINWIRE\",\"DimCompany\"],[\"FINWIRE\",\"DimSecurity\"],[\"FINWIRE\",\"Financial\"]]\n",
    "\n",
    "Modified_DimTable = [[\"CustomerMgmt.csv\",\"DimCustomer\"],[\"HR.csv\",\"DimBroker\"],\n",
    "                         [\"CustomerMgmt.csv\",\"DimAccount\"], [\"Trade.txt\",\"DimTrade\"]] \n",
    "\n",
    "\n",
    "Modified_FactTable = [[\"CashTransaction.txt\",\"FactCashBalances\"],[\"HoldingHistory.txt\",\"FactHoldings\"]\n",
    "                                  ,[\"DailyMarket.txt\",\"FactMarketHistory\"],[\"WatchHistory.txt\",\"FactWatches\"]\n",
    "                                  ,[\"Prospect.csv\",\"Prospect\"]]\n",
    "\n",
    "\n",
    "#How split FINWIRE FILES\n",
    "\n",
    "cmp_colspecs=[(0, 15), (15, 18), (18, 78), (78, 88), (88, 92), (92, 94), (94, 98), (98, 106), (106, 186), (186, 266), (266, 278), (278, 303), (303, 323), (323, 347), (347, 393), (393, 543)]\n",
    "fin_colspecs=[(0, 15), (15, 18), (18, 22), (22, 23), (23, 31), (31, 39), (39, 56), (56, 73), (73, 85), (85, 97), (97, 109), (109, 126), (126, 143), (143, 160), (160, 173), (173, 186), (186, 246)]\n",
    "sec_colspecs=[(0, 15), (15, 18), (18, 33), (33, 39), (39, 43), (43, 113), (113, 119), (119, 132), (132, 140), (140, 148), (148, 160), (160, 220)]\n",
    "\n",
    "\n",
    "\n",
    "# DIMESSAGES VALERIO\n",
    "\n",
    "valid_ratings = [\"AAA\", \"AA[+/-]\", \"A[+/-]\", \"BBB[+/-]\", \"BB[+/-]\", \"B[+/-]\", \"CCC[+/-]\", \"CC\", \"C\", \"D\"]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create final table in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:24:20 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:21 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/12/18 15:24:21 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/12/18 15:24:42 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/12/18 15:24:42 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore coucou@127.0.1.1\n",
      "23/12/18 15:24:42 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "23/12/18 15:24:44 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/12/18 15:24:44 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/12/18 15:24:44 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/12/18 15:24:44 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/12/18 15:24:44 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/dimaccount specified for non-external table:dimaccount\n",
      "23/12/18 15:24:45 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:45 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/dimbroker specified for non-external table:dimbroker\n",
      "23/12/18 15:24:46 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:46 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/dimcompany specified for non-external table:dimcompany\n",
      "23/12/18 15:24:46 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:46 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/dimcustomer specified for non-external table:dimcustomer\n",
      "23/12/18 15:24:46 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:46 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/dimdate specified for non-external table:dimdate\n",
      "23/12/18 15:24:46 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:46 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/dimsecurity specified for non-external table:dimsecurity\n",
      "23/12/18 15:24:46 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:47 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/dimtime specified for non-external table:dimtime\n",
      "23/12/18 15:24:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:47 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/dimtrade specified for non-external table:dimtrade\n",
      "23/12/18 15:24:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:47 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/dimessages specified for non-external table:dimessages\n",
      "23/12/18 15:24:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:47 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/factcashbalances specified for non-external table:factcashbalances\n",
      "23/12/18 15:24:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:47 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/factholdings specified for non-external table:factholdings\n",
      "23/12/18 15:24:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:47 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/factmarkethistory specified for non-external table:factmarkethistory\n",
      "23/12/18 15:24:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:47 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/factwatches specified for non-external table:factwatches\n",
      "23/12/18 15:24:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:48 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/industry specified for non-external table:industry\n",
      "23/12/18 15:24:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:48 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/financial specified for non-external table:financial\n",
      "23/12/18 15:24:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:48 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/prospect specified for non-external table:prospect\n",
      "23/12/18 15:24:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:48 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/statustype specified for non-external table:statustype\n",
      "23/12/18 15:24:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:48 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/taxrate specified for non-external table:taxrate\n",
      "23/12/18 15:24:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:48 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/tradetype specified for non-external table:tradetype\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:24:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "23/12/18 15:24:48 WARN HiveMetaStore: Location: file:/home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/spark-warehouse/audit specified for non-external table:audit\n"
     ]
    }
   ],
   "source": [
    "f = open(os.getcwd() + \"/ddl/tpcdi.sql\", \"r\")\n",
    "tables=f.read()\n",
    "tables = tables.split(\";\")[1:-1]\n",
    "for i in tables:\n",
    "    spark.sql(i)\n",
    "\n",
    "# AUDIT VALERIO\n",
    "# If not done: create the audit table in tpcdi.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIT VALERIO\n",
    "def visibility_2_fun():\n",
    "    \n",
    "    global start_time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    if execution_time > 300: # 300 seconds = 5 minutes\n",
    "        print(\"ERROR: more than 5 minutes elapsed between visibility queries's executions\")\n",
    "        \n",
    "    elif execution_time < 300 and execution_time > 120:\n",
    "        spark.sql(os.getcwd() + \"/ddl/tpcdi_visibility_2.sql\")\n",
    "        start_time = time.time()\n",
    "        return\n",
    "        \n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIT VALERIO\n",
    "# This function loads audit data into the audit table. It can be used for all the three batches.\n",
    "\n",
    "def audit_upload(batch_n):\n",
    "    # Go to Batch1 and consider only the files that end for \"audit.csv\"\n",
    "    audit_files = glob.glob(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch\"+ str(batch_n) +\"/*_audit.csv\")\n",
    "    \n",
    "    # For each audit_file...\n",
    "    sch_audit = spark.sql(\"select * from audit\").schema\n",
    "    for file_path in audit_files:\n",
    "        # Place \"audit_file\" data into a Spark DF\n",
    "        audit_df = spark.read.schema(sch_audit).csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Append data to the Audit table\n",
    "        audit_df.write.insertInto(\"Audit\", overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "/* ++++++++++++++++++++++++++++++++++++++++++++++++++ *\n",
      " * +                                                + *\n",
      " * +        TPC-DI  Validation Query                + *\n",
      " * +        Version 1.1.0                           + *\n",
      " * +                                                + *\n",
      " * ++++++++++++++++++++++++++++++++++++++++++++++++++ *\n",
      " *                                                    *\n",
      " *       ====== Portability Substitutions ======      *\n",
      " *     ---        [FROM DUMMY_TABLE]    ------        *\n",
      " * DB2            from sysibm.sysdummy1               *\n",
      " * ORACLE         from dual                           *\n",
      " * SQLSERVER       <blank>                            *\n",
      " * -------------------------------------------------- *\n",
      " *     ------  [||] (String concatenation) ------     *    \n",
      " * SQLSERVER      +                                   *\n",
      " * -------------------------------------------------- *\n",
      " */\n",
      "\n",
      "insert into DImessages\n",
      "\n",
      "select\n",
      "\n",
      "     CURRENT_TIMESTAMP as MessageDateAndTime\n",
      "     ,case when BatchID is null then 0 else BatchID end as BatchID\n",
      "     ,MessageSource\n",
      "     ,MessageText \n",
      "     ,'Validation' as MessageType\n",
      "     ,MessageData\n",
      "\n",
      "from (\n",
      "     select max(BatchID) as BatchID from DImessages \n",
      ") x join (\n",
      "\n",
      "    /* Basic row counts */\n",
      "     select 'DimAccount' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimAccount\n",
      "     union select 'DimBroker' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimBroker\n",
      "     union select 'DimCompany' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimCompany\n",
      "     union select 'DimCustomer' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimCustomer\n",
      "     union select 'DimDate' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimDate\n",
      "     union select 'DimSecurity' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimSecurity\n",
      "     union select 'DimTime' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimTime\n",
      "     union select 'DimTrade' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from DimTrade\n",
      "     union select 'FactCashBalances' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from FactCashBalances\n",
      "     union select 'FactHoldings' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from FactHoldings\n",
      "     union select 'FactMarketHistory' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from FactMarketHistory\n",
      "     union select 'FactWatches' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from FactWatches\n",
      "     union select 'Financial' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from Financial\n",
      "     union select 'Industry' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from Industry\n",
      "     union select 'Prospect' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from Prospect\n",
      "     union select 'StatusType' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from StatusType\n",
      "     union select 'TaxRate' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from TaxRate\n",
      "     union select 'TradeType' as MessageSource, 'Row count' as MessageText, count(*) as MessageData from TradeType\n",
      "     /* Joined row counts for Fact tables */\n",
      "     union select 'FactCashBalances' as MessageSource, 'Row count joined' as MessageText, \n",
      "\t\t\tcount(*) as MessageData \n",
      "\t\t\tfrom FactCashBalances f\n",
      "\t\t\tinner join DimAccount a on f.SK_AccountID = a.SK_AccountID\n",
      "\t\t\tinner join DimCustomer c on f.SK_CustomerID = c.SK_CustomerID\n",
      "\t\t\tinner join DimBroker b on a.SK_BrokerID = b.SK_BrokerID\n",
      "\t\t\tinner join DimDate d on f.SK_DateID = d.SK_DateID\n",
      "     union select 'FactHoldings' as MessageSource, 'Row count joined' as MessageText, \n",
      "\t\t\tcount(*) as MessageData \n",
      "\t\t\tfrom FactHoldings f\n",
      "\t\t\tinner join DimAccount a on f.SK_AccountID = a.SK_AccountID\n",
      "\t\t\tinner join DimCustomer c on f.SK_CustomerID = c.SK_CustomerID\n",
      "\t\t\tinner join DimBroker b on a.SK_BrokerID = b.SK_BrokerID\n",
      "\t\t\tinner join DimDate d on f.SK_DateID = d.SK_DateID\n",
      "\t\t\tinner join DimTime t on f.SK_TimeID = t.SK_TimeID\n",
      "\t\t\tinner join DimCompany m on f.SK_CompanyID = m.SK_CompanyID\n",
      "\t\t\tinner join DimSecurity s on f.SK_SecurityID = s.SK_SecurityID\n",
      "    union select 'FactMarketHistory' as MessageSource, 'Row count joined' as MessageText, \n",
      "\t\t\tcount(*) as MessageData \n",
      "\t\t\tfrom FactMarketHistory f\n",
      "\t\t\tinner join DimDate d on f.SK_DateID = d.SK_DateID\n",
      "\t\t\tinner join DimCompany m on f.SK_CompanyID = m.SK_CompanyID\n",
      "\t\t\tinner join DimSecurity s on f.SK_SecurityID = s.SK_SecurityID\n",
      "    union select 'FactWatches' as MessageSource, 'Row count joined' as MessageText, \n",
      "\t\t\tcount(*) as MessageData \n",
      "\t\t\tfrom FactWatches f\n",
      "\t\t\tinner join DimCustomer c on f.SK_CustomerID = c.SK_CustomerID\n",
      "\t\t\tinner join DimDate dp on f.SK_DateID_DatePlaced = dp.SK_DateID\n",
      "\t\t\t-- (cannot join on SK_DateID_DateRemoved because that field can be null)\n",
      "\t\t\tinner join DimSecurity s on f.SK_SecurityID = s.SK_SecurityID\n",
      "    /* Additional information used at Audit time */\n",
      "    union select 'DimCustomer' as MessageSource, 'Inactive customers' as MessageText, count(*) from DimCustomer where IsCurrent = 1 and Status = 'Inactive'\n",
      "    union select 'FactWatches' as MessageSource, 'Inactive watches' as MessageText, count(*) from FactWatches where SK_DATEID_DATEREMOVED is not null\n",
      ") y on 1=1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "-------------------------------\n",
      " \n",
      "/* Phase complete record */\n",
      "insert into DImessages\n",
      "select\n",
      "     MessageDateAndTime\n",
      "     ,case when BatchID is null then 0 else BatchID end as BatchID\n",
      "     ,MessageSource\n",
      "     ,MessageText \n",
      "     ,MessageType\n",
      "     ,MessageData\n",
      "from (\n",
      "     select CURRENT_TIMESTAMP as MessageDateAndTime\n",
      "            ,max(BatchID) as BatchID\n",
      "            ,'Phase Complete Record' as MessageSource\n",
      "            ,'Batch Complete' as MessageText\n",
      "            ,'PCR' as MessageType\n",
      "            ,NULL as MessageData\n",
      "  from DImessages\n",
      ") \n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AUDIT VALERIO\n",
    "# Just before startin the Historical Load: execute the batch validation query\n",
    "\n",
    "#I propose to create a new folders for all files.sql ??\n",
    "\n",
    "f = open(os.getcwd() + \"/ddl/tpcdi_validation.sql\", \"r\")\n",
    "tables=f.read()\n",
    "tables = tables.split(\";\")[:-1]\n",
    "for i in tables:\n",
    "    spark.sql(i)\n",
    "\n",
    "#spark.sql(os.getcwd() + \"/data/output/tpcdi_validation.sql\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HISTORYCAL LOAD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not_modified_table\n",
    "for i in Not_modified_table:\n",
    "    name_file = glob.glob(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch1/\"+ i[0] +\".txt\")\n",
    "    df_schema = spark.sql(\"select * from \" + i[1])\n",
    "    df = spark.read.schema(df_schema.schema).option(\"delimiter\", \"|\").csv(name_file)\n",
    "    df.write.mode(\"overwrite\").insertInto(i[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## FINWIRE FILES !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_date(string):\n",
    "    return string[0:4]+\"-\"+string[4:6]+\"-\"+string[6:8]\n",
    "    \n",
    "def string_to_timestamp(string):\n",
    "    return string[0:4]+\"-\"+string[4:6]+\"-\"+string[6:8]+\" \" +string[9:11]+\":\"+string[11:13]+\":\"+string[13:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finwire_files = [ finwire_source_files_path + '/' + file for file in os.listdir( finwire_source_files_path ) if re.search( r'^FINWIRE......$', file ) ] \n",
    "finwire_combined_df = pd.DataFrame()\n",
    "finwire_files = glob.glob(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch1/FINWIRE??????\")\n",
    "finwire_files.sort()\n",
    "\n",
    "#raise Exception(\"job done\")\n",
    "\n",
    "for file_path in finwire_files:\n",
    "    #Read the file and add to a combined Dataframe\n",
    "    c_names = [\"Content\"]\n",
    "    finwire_df = pd.read_csv(file_path, names=c_names,index_col=False)\n",
    "    finwire_df['Type'] = pd.DataFrame(finwire_df[\"Content\"].str.slice(15, 18))\n",
    "    finwire_combined_df = pd.concat([finwire_combined_df, finwire_df], ignore_index=True)\n",
    "    finwire_df_CMP=finwire_combined_df[finwire_combined_df[\"Type\"]=='CMP']\n",
    "    finwire_df_FIN=finwire_combined_df[finwire_combined_df[\"Type\"]=='FIN']\n",
    "    finwire_df_SEC=finwire_combined_df[finwire_combined_df[\"Type\"]=='SEC']\n",
    "    finwire_df_CMP=pd.DataFrame(finwire_df_CMP[[\"Content\"]])\n",
    "    finwire_df_FIN=pd.DataFrame(finwire_df_FIN[[\"Content\"]])\n",
    "    finwire_df_SEC=pd.DataFrame(finwire_df_SEC[[\"Content\"]])\n",
    "    \n",
    "    skiprows = 1\n",
    "    serrogate_key_CMP = 0\n",
    "    serrogate_key_SEC = 0\n",
    "    \n",
    "    \n",
    "    if finwire_df_CMP.empty  == False:\n",
    "        finwire_df_CMP.to_csv(\"CMP\", index=False)\n",
    "        df_CMP = pd.read_fwf(\"CMP\" , colspecs=cmp_colspecs, header = None, skiprows=skiprows)\n",
    "        \n",
    "        csv_file = open(os.getcwd() + \"/result/\" + file_path.split('/')[-1] + \"_CMP.csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        for index, row in df_CMP.iterrows():\n",
    "            #set value\n",
    "            Industry = spark.sql(\"select IN_NAME from INDUSTRY where IN_ID = '\" + row[5] + \"'\").collect()[0][0]\n",
    "            Status = spark.sql(\"select ST_NAME from StatusType where ST_ID = '\" + row[4] + \"'\").collect()[0][0]\n",
    "            # VALERIO DIMESSAGES\n",
    "            if row[6] not in valid_ratings:\n",
    "                df = [(current_timestamp(),1,\"DimCompany\",\"Invalid SPRating\",\"Alert\",f\"CO_ID = {row[3]}, CO_SP_RATE ={row[6]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                isLowGrade = None\n",
    "                row[6] = None\n",
    "                \n",
    "            else:                \n",
    "                if row[6].startswith('A') or row[6].startswith('BBB'):\n",
    "                    isLowGrade = 0\n",
    "                else:\n",
    "                    isLowGrade = 1\n",
    "            # ---\n",
    "            \n",
    "            #reorder\n",
    "            working_df = [serrogate_key_CMP,row[3],Status,row[2],Industry,row[6],isLowGrade,row[14],row[8],row[9]\n",
    "                          ,row[10],row[11],row[12],row[13],row[15],row[7],1,1\n",
    "                          ,string_to_timestamp(row[0]),'9999-12-31']\n",
    "            csv_writer.writerow(working_df)\n",
    "            serrogate_key_CMP += 1\n",
    "\n",
    "            \n",
    "        sch = spark.sql(\"select * from DimCompany\").schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/\" + file_path.split('/')[-1] + \"_CMP.csv\", sep=',')\n",
    "        df.write.mode(\"append\").insertInto(\"DimCompany\")\n",
    "         \n",
    "\n",
    "    \n",
    "    if finwire_df_SEC.empty == False:\n",
    "        finwire_df_SEC.to_csv(\"SEC\", index=False)\n",
    "        df_SEC = pd.read_fwf(\"SEC\" , colspecs=sec_colspecs, header = None, skiprows=skiprows)\n",
    "        \n",
    "        csv_file = open(os.getcwd() + \"/result/\" + file_path.split('/')[-1] + \"_SEC.csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        for index, row in df_SEC.iterrows():\n",
    "            #set value\n",
    "            SK_SECURITYID = spark.sql(\"\"\"select SK_CompanyID from DimCompany\n",
    "                        where (Name == '\"\"\"+ str(row[11]) +\"\"\"' or CompanyID== '\"\"\" \n",
    "                      + str(row[11]) + \"\"\"' ) and EffectiveDate <= '\"\"\" + string_to_timestamp(row[0]) +\n",
    "                      \"\"\"' and '\"\"\"+ string_to_timestamp(row[0]) + \"\"\"' < EndDate\"\"\").collect()[0][0]\n",
    "            print(row[4])\n",
    "            Status = spark.sql(\"select ST_NAME from StatusType where ST_ID = '\" + row[4] + \"'\").collect()[0][0]\n",
    "            \n",
    "            #reorder\n",
    "            working_df = [serrogate_key_SEC, row[2], row[3], Status, row[5], row[6], SK_SECURITYID, row[7], row[8]\n",
    "                         , row[9], row[10],1,1,string_to_timestamp(row[0]),'9999-12-31']\n",
    "            csv_writer.writerow(working_df)\n",
    "            serrogate_key_SEC += 1\n",
    "        \n",
    "        #add result\n",
    "        sch = spark.sql(\"select * from DimSecurity\").schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/\" + file_path.split('/')[-1] + \"_SEC.csv\", sep=',')\n",
    "        df.write.mode(\"append\").insertInto(\"DimSecurity\")\n",
    "        \n",
    "    if finwire_df_FIN.empty == False:\n",
    "        finwire_df_FIN.to_csv(\"FIN\", index=False)\n",
    "        df_FIN = pd.read_fwf(\"FIN\" , colspecs=fin_colspecs, header = None, skiprows=skiprows)\n",
    "        \n",
    "        csv_file = open(os.getcwd() + \"/result/\" + file_path.split('/')[-1] + \"_FIN.csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        for index, row in df_FIN.iterrows():\n",
    "            SK_COMPANYID =spark.sql(\"\"\"select SK_CompanyID from DimCompany\n",
    "                        where (Name == '\"\"\"+ str(row[16]) +\"\"\"' or CompanyID== '\"\"\" \n",
    "                      + str(row[16]) + \"\"\"' ) and EffectiveDate <= '\"\"\" + string_to_timestamp(row[0]) +\n",
    "                      \"\"\"' and '\"\"\"+ string_to_timestamp(row[0]) + \"\"\"' < EndDate\"\"\").collect()[0][0]\n",
    "                    \n",
    "            working_df = [SK_COMPANYID, row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9]\n",
    "                         , row[10], row[11], row[12], row[13], row[14], row[15]]\n",
    "            csv_writer.writerow(working_df)\n",
    "            \n",
    "        sch = spark.sql(\"select * from Financial\").schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/\" + file_path.split('/')[-1] + \"_FIN.csv\", sep=',')\n",
    "        df.write.mode(\"append\").insertInto(\"Financial\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRANSOFRMING customer.xml in customer.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch1/CustomerMgmt.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "def finder(obj_name):\n",
    "    try:\n",
    "        return Customer.find(f\".//{obj_name}\").text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# Create the .csv\n",
    "print(os.getcwd() + \"/CustomerMgmt.csv\")\n",
    "with open(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch1/CustomerMgmt.csv\", 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write the header\n",
    "    header = row = ['ActionType','ActionTS','C_ID', 'C_TAX_ID', 'C_GNDR', 'C_TIER', 'C_DOB', 'C_L_NAME', 'C_F_NAME', 'C_M_NAME', 'C_ADLINE1', 'C_ADLINE2', 'C_ZIPCODE', 'C_CITY', 'C_STATE_PROV', 'C_CTRY', 'C_PRIM_EMAIL', 'C_ALT_EMAIL', 'C_CTRY_CODE_1', 'C_AREA_CODE_1', 'C_LOCAL_1', 'C_EXT_1', 'C_CTRY_CODE_2', 'C_AREA_CODE_2', 'C_LOCAL_2', 'C_EXT_2', 'C_CTRY_CODE_3', 'C_AREA_CODE_3', 'C_LOCAL_3', 'C_EXT_3', 'C_LCL_TX_ID', 'C_NAT_TX_ID', 'CA_ID', 'CA_TAX_ST', 'CA_B_ID', 'CA_NAME']\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "    \n",
    "    # Iteratively add the customers\n",
    "    for TPCDAction in root:\n",
    "        row = []\n",
    "\n",
    "        action_type = TPCDAction.attrib.get(\"ActionType\")\n",
    "        action_ts = TPCDAction.attrib.get(\"ActionTS\")\n",
    "\n",
    "        for Customer in TPCDAction:\n",
    "            c_id = Customer.attrib.get(\"C_ID\")\n",
    "            c_tax_id = Customer.attrib.get(\"C_TAX_ID\")\n",
    "            c_gndr = Customer.attrib.get(\"C_GNDR\")\n",
    "            c_tier = Customer.attrib.get(\"C_TIER\")\n",
    "            c_dob = Customer.attrib.get(\"C_DOB\")\n",
    "            \n",
    "            c_l_name = finder(\"C_L_NAME\")\n",
    "            c_f_name = finder(\"C_F_NAME\")\n",
    "            c_m_name = finder(\"C_M_NAME\")\n",
    "            \n",
    "            c_adline1 = finder(\"C_ADLINE1\")\n",
    "            c_adline2 = finder(\"C_ADLINE2\")\n",
    "            c_zipcode = finder(\"C_ZIPCODE\")\n",
    "            c_city = finder(\"C_CITY\")\n",
    "            c_state_prov = finder(\"C_STATE_PROV\")\n",
    "            c_ctry = finder(\"C_CTRY\")\n",
    "        \n",
    "            c_prim_email = finder(\"C_PRIM_EMAIL\")\n",
    "            c_alt_email = finder(\"C_ALT_EMAIL\")\n",
    "            \n",
    "            c_ctry_code_1 = finder(\"C_PHONE_1//C_CTRY_CODE\")\n",
    "            c_area_code_1 = finder(\"C_PHONE_1//C_AREA_CODE\")\n",
    "            c_local_1 = finder(\"C_PHONE_1//C_LOCAL\")\n",
    "            c_ext_1 = finder(\"C_PHONE_1//C_EXT\")\n",
    "        \n",
    "            c_ctry_code_2 = finder(\"C_PHONE_2//C_CTRY_CODE\")\n",
    "            c_area_code_2 = finder(\"C_PHONE_2//C_AREA_CODE\")\n",
    "            c_local_2 = finder(\"C_PHONE_2//C_LOCAL\")\n",
    "            c_ext_2 = finder(\"C_PHONE_2//C_EXT\")\n",
    "\n",
    "            c_ctry_code_3 = finder(\"C_PHONE_3//C_CTRY_CODE\")\n",
    "            c_area_code_3 = finder(\"C_PHONE_3//C_AREA_CODE\")\n",
    "            c_local_3 = finder(\"C_PHONE_3//C_LOCAL\")\n",
    "            c_ext_3 = finder(\"C_PHONE_3//C_EXT\")\n",
    "            \n",
    "            c_lcl_tx_id = finder(f\"C_LCL_TX_ID\")\n",
    "            c_nat_tx_id = finder(f\"C_NAT_TX_ID\")\n",
    "            \n",
    "            try:\n",
    "                ca_id = Customer.find(\".//Account\").attrib.get(\"CA_ID\")\n",
    "            except:\n",
    "                ca_id = \"\"\n",
    "            \n",
    "            try:\n",
    "                ca_tax_st = Customer.find(\".//Account\").attrib.get(\"CA_TAX_ST\")\n",
    "            except:\n",
    "                ca_tax_st = \"\"\n",
    "            \n",
    "            ca_b_id = finder(f\"CA_B_ID\")\n",
    "            ca_name = finder(f\"CA_NAME\")\n",
    "        \n",
    "        row = [action_type, action_ts, c_id, c_tax_id, c_gndr, c_tier, c_dob, c_l_name, c_f_name, c_m_name, c_adline1, c_adline2, c_zipcode, c_city, c_state_prov, c_ctry, c_prim_email, c_alt_email, c_ctry_code_1, c_area_code_1, c_local_1, c_ext_1, c_ctry_code_2, c_area_code_2, c_local_2, c_ext_2, c_ctry_code_3, c_area_code_3, c_local_3, c_ext_3, c_lcl_tx_id, c_nat_tx_id, ca_id, ca_tax_st, ca_b_id, ca_name]\n",
    "        \n",
    "        csv_writer.writerow(row)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DIMENSION TABLE LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, trunc, upper, concat\n",
    "#Modified_table\n",
    "for i in Modified_DimTable:\n",
    "    name_file = glob.glob(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch1/\" +i[0])\n",
    "\n",
    "    if i[0] == \"HR.csv\":\n",
    "        csv_file = open(os.getcwd() + \"/result/Batch1_\" + i[1] +\".csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        hr_df = pd.read_csv(name_file[0], header=None,sep=r'[,|]')\n",
    "        broker_df = hr_df[hr_df[5]== 314] # Filter for brokers only\n",
    "        SK_BrokerID = 0\n",
    "        min_date = spark.sql(\"select min(DATEVALUE) from DimDate\")# Get the earliest date in the DimDate table\n",
    "        for index, row in broker_df.iterrows():\n",
    "            working_df = [SK_BrokerID,row[0],row[1],row[2],row[3],row[4],row[5],row[6],row[7],\n",
    "                         'True', \"9999-12-31\",min_date, '1']\n",
    "            SK_BrokerID +=1\n",
    "            csv_writer.writerow(working_df)\n",
    "                \n",
    "        sch = spark.sql(\"select * from \" + i[1]).schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch1_\" + i[1] +\".csv\", sep=',')\n",
    "        #df.write.mode(\"append\").insertInto(i[1])\n",
    "                \n",
    "    elif i[1] == \"DimCustomer\":\n",
    "        csv_file = open(os.getcwd() + \"/result/Batch1_\" + i[1] +\".csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "\n",
    "        df = pd.read_csv(\"CustomerMgmt.csv\", header=None,sep=r'[,|]')\n",
    "        \n",
    "        serrogate_key_Customer = 0\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if row[0] == 'NEW' or row[0] == 'UPDCUST':\n",
    "                Status = 'ACTIVE'\n",
    "            else:\n",
    "                Status = 'INACTIVE'\n",
    "            if row[4] == 'F' or  row[4] == 'H':\n",
    "                Gender = row[4]\n",
    "            else: \n",
    "                Gender ='U'\n",
    "            Phone1 = str(row[18])+str(row[19])+str(row[20])+str(row[21])\n",
    "            Phone2 = str(row[22])+str(row[23])+str(row[24])+str(row[25])\n",
    "            Phone3 = str(row[26])+str(row[27])+str(row[28])+str(row[29])\n",
    "\n",
    "            NationalTax = spark.sql(\"select TX_NAME, TX_RATE from TaxRate where TX_ID ='\" +str(row[31]) + \"'\")\n",
    "            NationalTaxRateDesc = NationalTax.collect()[0][0]\n",
    "            NationalTaxRate = NationalTax.collect()[0][1]\n",
    "\n",
    "            LocalTax = spark.sql(\"select TX_NAME, TX_RATE from TaxRate where TX_ID = '\" + str(row[30]) + \"'\")\n",
    "            LocalTaxRateDesc = LocalTax.collect()[0][0]\n",
    "            LocalTaxRate = LocalTax.collect()[0][1]\n",
    "\n",
    "            working_df = [serrogate_key_Customer, row[2],row[3],Status, row[7],row[8],row[9],Gender,row[5],row[6],\n",
    "                         row[10],row[11],row[12],row[13],row[14],row[15],Phone1,Phone2,Phone3,row[16],row[17]\n",
    "                         ,NationalTaxRateDesc,NationalTaxRate,LocalTaxRateDesc,LocalTaxRate,\n",
    "                          '','','','',1,1,row[1].split('T')[0],'9999-12-31']\n",
    "            csv_writer.writerow(working_df)\n",
    "            #raise Exception(\"Sorry, no numbers below zero\") \n",
    "            serrogate_key_Customer+=1\n",
    "            \n",
    "            # DIMESSAGES VALERIO\n",
    "            if row[5] not in [1,2,3]:\n",
    "                df = [(current_timestamp(),1,\"DimCustomer\",\"Invalid customer tier\",\"Alert\",f\"C_ID = {row[2]}, C_TIER ={row[5]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                \n",
    "            if row[6] < datetime(1923,11,15) or row[6] > (2023,11,15):\n",
    "                df = [(current_timestamp(),1,\"DimCustomer\",\"DOB out of range\",\"Alert\",f\"C_ID = {row[2]}, C_DOB ={row[6]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "            # ---\n",
    "        \n",
    "            \n",
    "        sch = spark.sql(\"select * from \" + i[1]).schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch1_\" + i[1] +\".csv\", sep=',')\n",
    "        #df.write.mode(\"append\").insertInto(i[1])\n",
    "                                        \n",
    "                                        \n",
    "    elif i[1] == \"DimAccount\":\n",
    "        csv_file = open(os.getcwd() + \"/result/Batch1_\" + i[1] +\".csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        df = pd.read_csv(\"CustomerMgmt.csv\", header=None,sep=r'[,|]')\n",
    "\n",
    "        serrogate_key_Account = 0\n",
    "        for index, row in df.iterrows():\n",
    "            action_type = row[0]\n",
    "            if action_type in ['NEW', 'ADDACCT', 'UPDACCT']:\n",
    "                Status = 'ACTIVE'\n",
    "            else:\n",
    "                Status = 'INACTIVE'\n",
    "\n",
    "            effective_date = row[1].replace(\"T\", \" \")\n",
    "            # Filter DimBroker to get the relevant BrokerID based on CA_B_ID and effective date\n",
    "            SK_BrokerID = spark.sql(\"select SK_BrokerID from DimBroker where EffectiveDate <= '\"+ effective_date \n",
    "                                    +\"' and EndDate >= '\"+ effective_date \n",
    "                                    +\"' and BrokerID == '\" + str(row[34]) + \"'\")#.collect()[0][0]\n",
    "\n",
    "            # Filter DimCustomer to get the relevant CustomerID based on C_ID and effective date\n",
    "            SK_CustomerID = spark.sql(\"select SK_CustomerID from DimCustomer where EffectiveDate <= '\"+ effective_date \n",
    "                                    +\"' and EndDate >= '\"+ effective_date \n",
    "                                    +\"' and CustomerID == '\" + str(row[2]) + \"'\")#.collect()[0][0]\n",
    "\n",
    "            working_df = [serrogate_key_Account, row[32], SK_BrokerID, SK_CustomerID, Status, row[35],\n",
    "                                  row[33], True, 1, effective_date, '9999-12-31']\n",
    "            \n",
    "            csv_writer.writerow(working_df)\n",
    "            serrogate_key_Account += 1\n",
    "        sch = spark.sql(\"select * from \" + i[1]).schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch1_\" + i[1] +\".csv\", sep=',')\n",
    "        #df.write.mode(\"append\").insertInto(i[1])\n",
    "        \n",
    "    elif i[1] == \"DimTrade\":\n",
    "        csv_file = open(os.getcwd() + \"/result/Batch1_\" + i[1] +\".csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        trade_data = pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch1/Trade.txt\", sep='|', header=None)\n",
    "        trade_history_data = pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch1/TradeHistory.txt\", sep='|', header=None)\n",
    "\n",
    "        trade_data.columns = ['T_ID', 'TH_DTS', 'TH_ST_ID', 'T_TT_ID', 'T_IS_CASH', 'T_S_SYMB', 'T_QTY', 'T_BID_PRICE', 'T_CA_ID', 'T_EXEC_NAME', 'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX']\n",
    "        trade_history_data.columns = ['T_ID', 'TH_DTS', 'TH_ST_ID']\n",
    "\n",
    "        #trade_data['TH_DTS'] = pd.to_datetime(trade_data['TH_DTS'], format='%Y-%m-%d %H:%M:%S')\n",
    "        #trade_history_data['TH_DTS'] = pd.to_datetime(trade_history_data['TH_DTS'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        for index, row in trade_data.iterrows():\n",
    "            time = spark.sql(\"select SK_DATEID from DimTime where SECONDDESC == '\"+row[1].split(\" \")[1] + \"'\").collect()[0][0]\n",
    "            date = spark.sql(\"select SK_DATEID from DimDate where DATEVALUE == '\"+row[1].split(\" \")[0] + \"'\").collect()[0][0]\n",
    "            \n",
    "            \n",
    "            if (row[2] == \"SBMT\" and (row[3] == \"TMB\" or row[3] == \"TMS\")) or row[2] == \"PNDG\":\n",
    "                SK_CreateDateID = date\n",
    "                SK_CreateTimeID = time\n",
    "                SK_CloseDateID = ''\n",
    "                SK_CloseTimeID = ''\n",
    "\n",
    "            elif row[2] in ['CMPT', 'CNCL']:\n",
    "                print(row[1])\n",
    "                SK_CreateDateID = ''\n",
    "                SK_CreateTimeID = ''\n",
    "                SK_CloseDateID = date\n",
    "                SK_CloseTimeID = time\n",
    "            tmp = '' #spark.sql(\"select SK_AccountID, SK_CustomerID, SK_BrokerID from DimAccount where AccountID = '\" \n",
    "                                  #       + row[?] + \"' and \"+row[1]+ \"between EffectiveDate and EndDate\" )\n",
    "            SK_BrokerID = '' #tmp.collect()[0][0]\n",
    "            SK_CustomerID = '' #tmp.collect()[0][1]\n",
    "            SK_AccountID = '' #tmp.collect()[0][2]\n",
    "            Status = '' #spark.sql(\"select ST_NAME from StatusType where ST_ID = '\" + row[?]+ \"'\" )\n",
    "            Type = '' #spark.sql(\"select TT_NAME from TradeType where TT_ID = '\" + row[?]+ \"'\" )\n",
    "            tmp = '' #spark.sql(\"select SK_SecurityID, SK_CompanyID from DimSecurity where SYMBOL = '\" + row[1] + \n",
    "                               #          \"' and EFFECTIVEDATE =\"+row[0] +\"'\")\n",
    "            SK_SecurityID = '' #tmp.collect()[0][0]\n",
    "            SK_CompanyID = '' #tmp.collect()[0][1]\n",
    "            \n",
    "            \n",
    "            #Pour tout ce qui est SK, ainsi que Status et Type y a juste à faire le match avec les autres tables et récupérer les valeurs\n",
    "            working_df = [row[0], SK_BrokerID, SK_CreateDateID, SK_CreateTimeID, SK_CloseDateID,\n",
    "                                          SK_CloseTimeID, Status, Type, row[4], SK_SecurityID, SK_CompanyID,\n",
    "                                          row[6], row[7], SK_CustomerID, SK_AccountID, row[9], row[10], row[11],\n",
    "                                          row[12], row[13], 1]\n",
    "            csv_writer.writerow(working_df_trade)\n",
    "            \n",
    "            # DIMESSAGES VALERIO\n",
    "            if row[17] is not None and row[17] > (row[16] * row[11]):\n",
    "                df = [(current_timestamp(),1,\"DimTrade\",\"Invalid trade fee\",\"Alert\",f\"T_ID = {row[0]}, T_CHRG ={row[17]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                \n",
    "            if row[18] is not None and row[18] > (row[16] * row[11]):\n",
    "                df = [(current_timestamp(),1,\"DimTrade\",\"Invalid trade commission\",\"Alert\",f\"T_ID = {row[0]}, T_COMM ={row[18]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "            # ---\n",
    "                                        \n",
    "                                        \n",
    "        sch = spark.sql(\"select * from \" + i[1]).schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch1_\" + i[1] +\".csv\", sep=',')\n",
    "        #df.write.mode(\"append\").insertInto(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACT TABLE LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in Modified_FactTable:\n",
    "    name_file = glob.glob(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch1/\" +i[0])\n",
    "    print(i[0])\n",
    "    if i[1] == \"FactCashBalances\" or False:\n",
    "        csv_file = open(os.getcwd() + \"/result/\" + i[1] + \".csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "\n",
    "        df = pd.read_csv(name_file[0], header=None,sep=r'[,|]')\n",
    "        for index, row in df.iterrows():\n",
    "            working_df = []\n",
    "            if i[1] == \"FactCashBalances\":\n",
    "                #set value\n",
    "                info_dim=spark.sql(f\"\"\"SELECT SK_CustomerID,SK_AccountID FROM DimAccount\n",
    "                                        WHERE ACCOUNTID = {row[0]} AND EFFECTIVEDATE <= '{row[1].split(\" \")[0]}'\n",
    "                                        AND '{row[1].split(\" \")[0]}' <= ENDDATE\"\"\")\n",
    "\n",
    "                SK_CustomerID=info_dim.collect()[0][0]\n",
    "                SK_AccountID=info_dim.collect()[0][1]\n",
    "                SK_DateID = spark.sql(f\"\"\"SELECT SK_DateID FROM DimDate\n",
    "                                        where DateValue = '{row[1].split(\" \")[0]}' \"\"\").collect()[0][0]\n",
    "                Cash = 0 # spark.sql(\"select SK_DateID from DimDate where DateValue = '\" + row[1] + \"'\")\n",
    "                BatchID =1\n",
    "\n",
    "                working_df = [SK_CustomerID,SK_AccountID,SK_DateID,Cash,BatchID]\n",
    "            elif i[1] == \"FactMarketHistory\":\n",
    "                #set value\n",
    "                tmp = '' #spark.sql(\"select SK_CustomerID, SK_CompanyID from DimSecurity where Symbol = '\" + row[3] + \" where IsCurrent = '1'\")\n",
    "                SK_SECURITYID = '' #tmp.collect()[0][0]\n",
    "                SK_COMPANYID = '' #tmp.collect()[0][1]\n",
    "                SK_DATEID= '' #spark.sql(\"select SK_DATEID from DimDate where DateValue = '\" + row[0] + \"'\")\n",
    "                PERATIO= '' # sprak ????\n",
    "                YIELD= '' #spark.sql(\"select dividend from DimSecurity where SYMBOL = '\" + row[1] + \"'and \"+row[0]+\n",
    "                       #          \"between EffectiveDate and EndDate\")/row[2]\n",
    "                FIFTYTWOWEEKHIGH = ''#spark.sql(\"select max(DM_HIGH) from FactMarketHistory \n",
    "                        #where\" + row[0] +\" <> date_sub(\"+row[0]+\", 365))\"\n",
    "                FIFTYTWOWEEKLOW= '' #spark.sql(\"select min(DM_LOW) from FactMarketHistory \n",
    "                                #where\" + row[0] +\" <> date_sub(\"+row[0]+\", 365))\"\n",
    "                SK_FIFTYTWOWEEKLOWDATE = '' # ??\n",
    "                SK_FIFTYTWOWEEKHIGHDATE = '' #??                                                         #         \" from tmp where\" + row[0] +\" <> date_sub(\"+row[0]+\", 365))\"\n",
    "\n",
    "                CLOSEPRICE=row[2]\n",
    "                DAYHIGH=row[3]\n",
    "                DAYLOW=row[4]\n",
    "                VOLUME=row[5]\n",
    "                BATCHID=1\n",
    "\n",
    "                working_df = [SK_SecurityID,SK_COMPANYID,SK_DATEID,PERATIO,YIELD,FIFTYTWOWEEKHIGH,\n",
    "                              SK_FIFTYTWOWEEKHIGHDATE,FIFTYTWOWEEKLOW,SK_FIFTYTWOWEEKLOWDATE,\n",
    "                              CLOSEPRICE,DAYHIGH,DAYLOW,VOLUME,BATCHID]\n",
    "                \n",
    "                # DIMESSAGES VALERIO\n",
    "                if row[3] is None:\n",
    "                     df = [(current_timestamp(),1,\"FactMarketHistory\",\"No earnings for company\",\"Alert\",f\"DM_S_SYMB={row[3]}\")]\n",
    "                     df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                # ---\n",
    "                \n",
    "            elif i[1] == \"FactWatches\":\n",
    "                #SK_CUSTOMERID = 0#spark.sql(\"select SK_CustomerID from DimCustomer where CUSTOMERID = '\" + row[0] + \"'\")\n",
    "                #SK_SECURITYID = 0#spark.sql(\"select SK_SecurityID from DimSecurity where SYMBOL = '\" + row[1] + \"'\")\n",
    "                \n",
    "                SK_CUSTOMERID = spark.sql(f\"\"\"\n",
    "                                    SELECT SK_CUSTOMERID\n",
    "                                    FROM DimCustomer\n",
    "                                    WHERE CUSTOMERID = {row[0]} \"\"\").collect()[0][0] \n",
    "\n",
    "\n",
    "\n",
    "                SK_SECURITYID = spark.sql(f\"\"\"\n",
    "                                    SELECT SK_SecurityID\n",
    "                                    FROM DimSecurity\n",
    "                                    WHERE SYMBOL = '{row[1]}' \"\"\").collect()[0][0] \n",
    "                SK_DATEID_DATEPLACED=row[2]\n",
    "                if row[3]==\"ACTV\":\n",
    "                    SK_DATEID_DATEREMOVED = ''\n",
    "                else:\n",
    "                    SK_DATEID_DATEREMOVED = row[2]\n",
    "                BATCHID=1\n",
    "\n",
    "                working_df = [SK_CUSTOMERID,SK_SECURITYID,SK_DATEID_DATEPLACED,SK_DATEID_DATEREMOVED,BATCHID]\n",
    "\n",
    "\n",
    "            elif i[1] == \"FactHoldings\":\n",
    "                print(row)\n",
    "                TRADEID = row[0]\n",
    "                CURRENTTRADEID = row[1]\n",
    "                df = spark.sql(\"\"\"select SK_CustomerID,SK_AccountID,SK_SECURITYID,SK_COMPANYID,\n",
    "                SK_CLOSEDATEID,SK_CloseTimeID, SK_AccountID\n",
    "                               from DimTrade where TradeID = '\"\"\" + str(row[1]) + \"'\")\n",
    "                \n",
    "                \n",
    "                SK_CUSTOMERID =df.collect()[0][0]\n",
    "                SK_ACCOUNTID = df.collect()[0][1]\n",
    "                SK_SECURITYID = df.collect()[0][2]\n",
    "                SK_COMPANYID = df.collect()[0][3]\n",
    "                SK_DATEID =df.collect()[0][4]\n",
    "                SK_TIMEID = df.collect()[0][5]\n",
    "                CURRENTPRICE =df.collect()[0][6]\n",
    "                CURRENTHOLDING = row[3]\n",
    "                BATCHID = 1                                                            \n",
    "                working_df = [TRADEID,CURRENTTRADEID,SK_CUSTOMERID,SK_ACCOUNTID,SK_SECURITYID,SK_COMPANYID,\n",
    "                              SK_DATEID,SK_TIMEID,CURRENTPRICE,CURRENTHOLDING,BATCHID]\n",
    "\n",
    "            elif i[1] == \"Prospect\":\n",
    "                isCustomer = spark.sql(\"select * from DimCustomer where status = 'ACTIVE' \")#to-do\n",
    "                if isCustomer.count() == 0:\n",
    "                    isCustomer = 0\n",
    "                else:\n",
    "                    isCustomer = 1\n",
    "                SK_RecordDateID = 0 #to-do\n",
    "                SK_UpdateDateID = 0 #to-do\n",
    "                \n",
    "                MarketingNameplate = \"\"\n",
    "                if row[12] > 200000 or row[21] > 1000000:\n",
    "                    MarketingNameplate += \"+HighValue\"\n",
    "                if row[14] > 3 or row[20] > 5:\n",
    "                    MarketingNameplate += \"+Expenses\"\n",
    "                if row[16] > 45:\n",
    "                    MarketingNameplate += \"+Boomer\"\n",
    "                if row[12] < 100000 or row[21] < 50000 or row[17] < 600:\n",
    "                    MarketingNameplate += \"+MoneyAlert\"\n",
    "                if row[13] > 3 or row[20] > 7 :\n",
    "                    MarketingNameplate += \"+Spender\"\n",
    "                if row[16] < 25 and row[12] > 1000000 :\n",
    "                    MarketingNameplate += \"+Inherited\"\n",
    "                if len(MarketingNameplate) != 0:\n",
    "                    MarketingNameplate = MarketingNameplate[1:]\n",
    "                \n",
    "                \n",
    "                working_df = [row[0],SK_RecordDateID,SK_UpdateDateID,1,isCustomer\n",
    "                              ,row[1],row[2],row[3],row[4],row[5],row[6]\n",
    "                              ,row[7],row[8],row[9],row[10]\n",
    "                             ,row[11],row[12],row[13],row[14],row[15],row[16],\n",
    "                              row[17],row[18],row[19],row[20],row[21],MarketingNameplate]\n",
    "                print(MarketingNameplate)\n",
    "\n",
    "            csv_writer.writerow(working_df)\n",
    "\n",
    "        #add result \n",
    "        sch = spark.sql(\"select * from \" + i[1]).schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/\" + i[1] + \".csv\", sep=',')\n",
    "        ·#df.write.mode(\"append\").insertInto(i[1])\n",
    "        \n",
    "        if i[1] == \"Prospect\":\n",
    "            df = [(current_timestamp(),1,\"Prospect\",\"Inserted rows\",\"Status\",prospect_count)]\n",
    "            df.write.mode(\"append\").insertInto(\"DImessages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:29:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2002Q2_audit.csv\n",
      "23/12/18 15:29:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1984Q3_audit.csv\n",
      "23/12/18 15:29:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1998Q4_audit.csv\n",
      "23/12/18 15:29:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1978Q4_audit.csv\n",
      "23/12/18 15:29:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1997Q2_audit.csv\n",
      "23/12/18 15:29:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2013Q2_audit.csv\n",
      "23/12/18 15:29:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1986Q1_audit.csv\n",
      "23/12/18 15:29:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1990Q2_audit.csv\n",
      "23/12/18 15:29:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2009Q1_audit.csv\n",
      "23/12/18 15:30:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1989Q3_audit.csv\n",
      "23/12/18 15:30:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1976Q1_audit.csv\n",
      "23/12/18 15:30:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1968Q4_audit.csv\n",
      "23/12/18 15:30:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1971Q2_audit.csv\n",
      "23/12/18 15:30:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1991Q1_audit.csv\n",
      "23/12/18 15:30:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1993Q2_audit.csv\n",
      "23/12/18 15:30:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1972Q2_audit.csv\n",
      "23/12/18 15:30:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1973Q2_audit.csv\n",
      "23/12/18 15:30:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1969Q2_audit.csv\n",
      "23/12/18 15:30:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1997Q3_audit.csv\n",
      "23/12/18 15:30:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2015Q1_audit.csv\n",
      "23/12/18 15:30:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2010Q3_audit.csv\n",
      "23/12/18 15:30:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1989Q1_audit.csv\n",
      "23/12/18 15:30:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1988Q4_audit.csv\n",
      "23/12/18 15:30:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/HR_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:30:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2006Q1_audit.csv\n",
      "23/12/18 15:30:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1990Q1_audit.csv\n",
      "23/12/18 15:30:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/TaxRate_audit.csv\n",
      "23/12/18 15:30:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1984Q2_audit.csv\n",
      "23/12/18 15:30:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2012Q2_audit.csv\n",
      "23/12/18 15:30:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/DailyMarket_audit.csv\n",
      "23/12/18 15:30:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1983Q4_audit.csv\n",
      "23/12/18 15:30:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1981Q3_audit.csv\n",
      "23/12/18 15:30:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2009Q3_audit.csv\n",
      "23/12/18 15:30:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2017Q3_audit.csv\n",
      "23/12/18 15:30:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1978Q1_audit.csv\n",
      "23/12/18 15:30:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1985Q2_audit.csv\n",
      "23/12/18 15:30:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1977Q2_audit.csv\n",
      "23/12/18 15:30:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2013Q1_audit.csv\n",
      "23/12/18 15:30:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1991Q4_audit.csv\n",
      "23/12/18 15:30:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1998Q3_audit.csv\n",
      "23/12/18 15:30:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1999Q4_audit.csv\n",
      "23/12/18 15:30:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1968Q2_audit.csv\n",
      "23/12/18 15:30:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1994Q1_audit.csv\n",
      "23/12/18 15:30:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1993Q1_audit.csv\n",
      "23/12/18 15:30:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1975Q4_audit.csv\n",
      "23/12/18 15:30:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1977Q4_audit.csv\n",
      "23/12/18 15:30:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1999Q2_audit.csv\n",
      "23/12/18 15:30:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1979Q2_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:30:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1996Q2_audit.csv\n",
      "23/12/18 15:30:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1978Q3_audit.csv\n",
      "23/12/18 15:30:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2000Q3_audit.csv\n",
      "23/12/18 15:30:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1967Q3_audit.csv\n",
      "23/12/18 15:30:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2002Q1_audit.csv\n",
      "23/12/18 15:30:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1999Q1_audit.csv\n",
      "23/12/18 15:30:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2010Q4_audit.csv\n",
      "23/12/18 15:30:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2003Q4_audit.csv\n",
      "23/12/18 15:30:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2015Q4_audit.csv\n",
      "23/12/18 15:30:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1981Q1_audit.csv\n",
      "23/12/18 15:30:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2000Q1_audit.csv\n",
      "23/12/18 15:30:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2014Q2_audit.csv\n",
      "23/12/18 15:30:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2011Q1_audit.csv\n",
      "23/12/18 15:30:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2004Q3_audit.csv\n",
      "23/12/18 15:30:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2001Q1_audit.csv\n",
      "23/12/18 15:30:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1975Q1_audit.csv\n",
      "23/12/18 15:30:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1971Q1_audit.csv\n",
      "23/12/18 15:30:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2008Q2_audit.csv\n",
      "23/12/18 15:30:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1975Q3_audit.csv\n",
      "23/12/18 15:30:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1988Q3_audit.csv\n",
      "23/12/18 15:30:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/TradeHistory_audit.csv\n",
      "23/12/18 15:30:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1967Q2_audit.csv\n",
      "23/12/18 15:30:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1979Q3_audit.csv\n",
      "23/12/18 15:30:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1994Q3_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:30:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1997Q4_audit.csv\n",
      "23/12/18 15:30:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1992Q2_audit.csv\n",
      "23/12/18 15:30:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1974Q3_audit.csv\n",
      "23/12/18 15:30:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2005Q4_audit.csv\n",
      "23/12/18 15:30:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2007Q3_audit.csv\n",
      "23/12/18 15:30:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1981Q2_audit.csv\n",
      "23/12/18 15:30:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/Date_audit.csv\n",
      "23/12/18 15:30:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1988Q2_audit.csv\n",
      "23/12/18 15:30:23 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1978Q2_audit.csv\n",
      "23/12/18 15:30:23 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2001Q2_audit.csv\n",
      "23/12/18 15:30:23 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/WatchHistory_audit.csv\n",
      "23/12/18 15:30:23 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1969Q3_audit.csv\n",
      "23/12/18 15:30:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1980Q1_audit.csv\n",
      "23/12/18 15:30:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1985Q4_audit.csv\n",
      "23/12/18 15:30:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1973Q1_audit.csv\n",
      "23/12/18 15:30:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2002Q4_audit.csv\n",
      "23/12/18 15:30:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1986Q2_audit.csv\n",
      "23/12/18 15:30:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/TradeType_audit.csv\n",
      "23/12/18 15:30:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1970Q4_audit.csv\n",
      "23/12/18 15:30:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2008Q4_audit.csv\n",
      "23/12/18 15:30:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1993Q3_audit.csv\n",
      "23/12/18 15:30:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2000Q2_audit.csv\n",
      "23/12/18 15:30:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1976Q4_audit.csv\n",
      "23/12/18 15:30:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1967Q1_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:30:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1999Q3_audit.csv\n",
      "23/12/18 15:30:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2003Q3_audit.csv\n",
      "23/12/18 15:30:27 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1972Q4_audit.csv\n",
      "23/12/18 15:30:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2017Q2_audit.csv\n",
      "23/12/18 15:30:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2012Q4_audit.csv\n",
      "23/12/18 15:30:28 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1995Q3_audit.csv\n",
      "23/12/18 15:30:29 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1991Q2_audit.csv\n",
      "23/12/18 15:30:29 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1980Q4_audit.csv\n",
      "23/12/18 15:30:29 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2001Q4_audit.csv\n",
      "23/12/18 15:30:29 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2010Q2_audit.csv\n",
      "23/12/18 15:30:30 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2016Q2_audit.csv\n",
      "23/12/18 15:30:30 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2011Q4_audit.csv\n",
      "23/12/18 15:30:30 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1969Q4_audit.csv\n",
      "23/12/18 15:30:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2009Q4_audit.csv\n",
      "23/12/18 15:30:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1975Q2_audit.csv\n",
      "23/12/18 15:30:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2006Q2_audit.csv\n",
      "23/12/18 15:30:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2012Q3_audit.csv\n",
      "23/12/18 15:30:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2004Q1_audit.csv\n",
      "23/12/18 15:30:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1981Q4_audit.csv\n",
      "23/12/18 15:30:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1970Q3_audit.csv\n",
      "23/12/18 15:30:33 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1987Q2_audit.csv\n",
      "23/12/18 15:30:33 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2003Q1_audit.csv\n",
      "23/12/18 15:30:33 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1992Q4_audit.csv\n",
      "23/12/18 15:30:33 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1983Q1_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:30:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2003Q2_audit.csv\n",
      "23/12/18 15:30:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1973Q4_audit.csv\n",
      "23/12/18 15:30:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1995Q2_audit.csv\n",
      "23/12/18 15:30:34 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1976Q2_audit.csv\n",
      "23/12/18 15:30:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1971Q3_audit.csv\n",
      "23/12/18 15:30:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2005Q1_audit.csv\n",
      "23/12/18 15:30:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/Trade_audit.csv\n",
      "23/12/18 15:30:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1988Q1_audit.csv\n",
      "23/12/18 15:30:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2016Q4_audit.csv\n",
      "23/12/18 15:30:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1973Q3_audit.csv\n",
      "23/12/18 15:30:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/StatusType_audit.csv\n",
      "23/12/18 15:30:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1968Q1_audit.csv\n",
      "23/12/18 15:30:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1974Q2_audit.csv\n",
      "23/12/18 15:30:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/Time_audit.csv\n",
      "23/12/18 15:30:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1998Q2_audit.csv\n",
      "23/12/18 15:30:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2013Q4_audit.csv\n",
      "23/12/18 15:30:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2006Q4_audit.csv\n",
      "23/12/18 15:30:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2007Q2_audit.csv\n",
      "23/12/18 15:30:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2011Q2_audit.csv\n",
      "23/12/18 15:30:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1992Q3_audit.csv\n",
      "23/12/18 15:30:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2005Q3_audit.csv\n",
      "23/12/18 15:30:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/Industry_audit.csv\n",
      "23/12/18 15:30:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1987Q1_audit.csv\n",
      "23/12/18 15:30:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2001Q3_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:30:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2017Q1_audit.csv\n",
      "23/12/18 15:30:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2006Q3_audit.csv\n",
      "23/12/18 15:30:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1968Q3_audit.csv\n",
      "23/12/18 15:30:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1983Q3_audit.csv\n",
      "23/12/18 15:30:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1987Q3_audit.csv\n",
      "23/12/18 15:30:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1970Q2_audit.csv\n",
      "23/12/18 15:30:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1979Q4_audit.csv\n",
      "23/12/18 15:30:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1989Q4_audit.csv\n",
      "23/12/18 15:30:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1991Q3_audit.csv\n",
      "23/12/18 15:30:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1995Q1_audit.csv\n",
      "23/12/18 15:30:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2004Q2_audit.csv\n",
      "23/12/18 15:30:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1994Q2_audit.csv\n",
      "23/12/18 15:30:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2014Q1_audit.csv\n",
      "23/12/18 15:30:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1980Q3_audit.csv\n",
      "23/12/18 15:30:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1982Q1_audit.csv\n",
      "23/12/18 15:30:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1996Q1_audit.csv\n",
      "23/12/18 15:30:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1996Q4_audit.csv\n",
      "23/12/18 15:30:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1971Q4_audit.csv\n",
      "23/12/18 15:30:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1972Q3_audit.csv\n",
      "23/12/18 15:30:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2014Q4_audit.csv\n",
      "23/12/18 15:30:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1980Q2_audit.csv\n",
      "23/12/18 15:30:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1977Q3_audit.csv\n",
      "23/12/18 15:30:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1997Q1_audit.csv\n",
      "23/12/18 15:30:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1989Q2_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:30:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1985Q1_audit.csv\n",
      "23/12/18 15:30:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2016Q1_audit.csv\n",
      "23/12/18 15:30:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1982Q4_audit.csv\n",
      "23/12/18 15:30:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2012Q1_audit.csv\n",
      "23/12/18 15:30:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1972Q1_audit.csv\n",
      "23/12/18 15:30:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2014Q3_audit.csv\n",
      "23/12/18 15:30:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/CustomerMgmt_audit.csv\n",
      "23/12/18 15:30:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1993Q4_audit.csv\n",
      "23/12/18 15:30:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1974Q1_audit.csv\n",
      "23/12/18 15:30:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1995Q4_audit.csv\n",
      "23/12/18 15:30:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1987Q4_audit.csv\n",
      "23/12/18 15:30:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1967Q4_audit.csv\n",
      "23/12/18 15:30:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1970Q1_audit.csv\n",
      "23/12/18 15:30:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1992Q1_audit.csv\n",
      "23/12/18 15:30:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2007Q4_audit.csv\n",
      "23/12/18 15:30:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2016Q3_audit.csv\n",
      "23/12/18 15:30:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1990Q3_audit.csv\n",
      "23/12/18 15:30:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2008Q3_audit.csv\n",
      "23/12/18 15:30:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1974Q4_audit.csv\n",
      "23/12/18 15:30:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/CashTransaction_audit.csv\n",
      "23/12/18 15:30:51 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2004Q4_audit.csv\n",
      "23/12/18 15:30:51 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1979Q1_audit.csv\n",
      "23/12/18 15:30:51 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2015Q2_audit.csv\n",
      "23/12/18 15:30:51 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/HoldingHistory_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:30:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2015Q3_audit.csv\n",
      "23/12/18 15:30:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/Prospect_audit.csv\n",
      "23/12/18 15:30:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1986Q3_audit.csv\n",
      "23/12/18 15:30:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1977Q1_audit.csv\n",
      "23/12/18 15:30:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1985Q3_audit.csv\n",
      "23/12/18 15:30:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1998Q1_audit.csv\n",
      "23/12/18 15:30:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1982Q3_audit.csv\n",
      "23/12/18 15:30:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1983Q2_audit.csv\n",
      "23/12/18 15:30:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1996Q3_audit.csv\n",
      "23/12/18 15:30:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1990Q4_audit.csv\n",
      "23/12/18 15:30:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1982Q2_audit.csv\n",
      "23/12/18 15:30:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2008Q1_audit.csv\n",
      "23/12/18 15:30:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1969Q1_audit.csv\n",
      "23/12/18 15:30:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1976Q3_audit.csv\n",
      "23/12/18 15:30:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1984Q4_audit.csv\n",
      "23/12/18 15:30:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1986Q4_audit.csv\n",
      "23/12/18 15:30:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1984Q1_audit.csv\n",
      "23/12/18 15:30:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2010Q1_audit.csv\n",
      "23/12/18 15:30:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2005Q2_audit.csv\n",
      "23/12/18 15:30:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE1994Q4_audit.csv\n",
      "23/12/18 15:30:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2007Q1_audit.csv\n",
      "23/12/18 15:30:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2000Q4_audit.csv\n",
      "23/12/18 15:30:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2009Q2_audit.csv\n",
      "23/12/18 15:30:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2002Q3_audit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 15:30:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2013Q3_audit.csv\n",
      "23/12/18 15:30:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DataSet,  BatchID , Date ,  Attribute ,  Value,  DValue\n",
      " Schema: DATASET, BATCHID, DATE, ATTRIBUTE, VALUE, DVALUE\n",
      "Expected: BATCHID but found:  BatchID \n",
      "CSV file: file:///home/coucou/ULB/MA2/DataWarehouse/Projet/TPC-DI/data/output/Batch1/FINWIRE2011Q3_audit.csv\n"
     ]
    }
   ],
   "source": [
    "# AUDIT VALERIO\n",
    "# Execute the following line during the Historical Phase\n",
    "audit_upload(1)\n",
    "\n",
    "# At the end of the Historical Load: execute the batch validation query\n",
    "#spark.sql(os.getcwd() + \"/data/output/tpcdi_validation.sql\")\n",
    "\n",
    "f = open(os.getcwd() + \"/ddl/tpcdi_validation.sql\", \"r\")\n",
    "tables=f.read()\n",
    "tables = tables.split(\";\")[:-1]\n",
    "for i in tables:\n",
    "    spark.sql(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+----+----------+-------+------+\n",
      "|          DATASET|BATCHID|DATE| ATTRIBUTE|  VALUE|DVALUE|\n",
      "+-----------------+-------+----+----------+-------+------+\n",
      "|      DimSecurity|      1|NULL|    FW_SEC|     19|  NULL|\n",
      "|      DimSecurity|      1|NULL|FW_SEC_DUP|     -1|  NULL|\n",
      "|       DimCompany|      1|NULL|    FW_CMP|     12|  NULL|\n",
      "|       DimCompany|      1|NULL|FW_CMP_DUP|     -1|  NULL|\n",
      "|        Financial|      1|NULL|    FW_FIN|    994|  NULL|\n",
      "|        Financial|      1|NULL|FW_FIN_DUP|     -1|  NULL|\n",
      "|FactMarketHistory|      1|NULL|DM_RECORDS|2422064|  NULL|\n",
      "|      DimSecurity|      1|NULL|    FW_SEC|     19|  NULL|\n",
      "|      DimSecurity|      1|NULL|FW_SEC_DUP|     -1|  NULL|\n",
      "|       DimCompany|      1|NULL|    FW_CMP|     12|  NULL|\n",
      "|       DimCompany|      1|NULL|FW_CMP_DUP|     -1|  NULL|\n",
      "|        Financial|      1|NULL|    FW_FIN|   1849|  NULL|\n",
      "|        Financial|      1|NULL|FW_FIN_DUP|     -1|  NULL|\n",
      "|      DimSecurity|      1|NULL|    FW_SEC|     19|  NULL|\n",
      "|      DimSecurity|      1|NULL|FW_SEC_DUP|     -1|  NULL|\n",
      "|       DimCompany|      1|NULL|    FW_CMP|     12|  NULL|\n",
      "|       DimCompany|      1|NULL|FW_CMP_DUP|     -1|  NULL|\n",
      "|        Financial|      1|NULL|    FW_FIN|   1615|  NULL|\n",
      "|        Financial|      1|NULL|FW_FIN_DUP|     -1|  NULL|\n",
      "|      DimSecurity|      1|NULL|    FW_SEC|     19|  NULL|\n",
      "+-----------------+-------+----+----------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from audit\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIT VALERIO\n",
    "# Run Visibility 1 at the start of incremental\n",
    "#spark.sql(os.getcwd() + \"/data/output/tpcdi_visibility_1.sql\")\n",
    "\n",
    "f = open(os.getcwd() + \"/ddl/tpcdi_visibility_1.sql\", \"r\")\n",
    "tables=f.read()\n",
    "tables = tables.split(\";\")[:-1]\n",
    "for i in tables:\n",
    "    spark.sql(i)\n",
    "\n",
    "# Run Visibility 2 right after visibility 1\n",
    "#spark.sql(os.getcwd() + \"/data/output/tpcdi_visibility_2.sql\")\n",
    "\n",
    "f = open(os.getcwd() + \"/tpcdi_visibility_2.sql\", \"r\")\n",
    "tables=f.read()\n",
    "tables = tables.split(\";\")[:-1]\n",
    "for i in tables:\n",
    "    spark.sql(i)\n",
    "\n",
    "# Register time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incr_DimAccount(batch_id):\n",
    "    csv_file = open(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_DimAccount.csv\", 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    account_data = pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch\"+str(batch_id) +\"/Account.txt\", header=None,sep=r'[,|]')\n",
    "    \n",
    "    account_data.columns = ['I/U', 'CA_ID', 'CA_NAME', 'CA_TAX_ST', 'CA_B_ID', 'CA_C_ID', 'CA_ST_ID', 'CDC_FLAG']\n",
    "    serrogate_key_Account = 0#spark.sql(\"select max(SK_ACCOUNTID) from DimAccount\").collect()[0][0]\n",
    "    print(account_data)\n",
    "    for index, row in account_data.iterrows():\n",
    "        if row[0] == \"I\":\n",
    "            pass\n",
    "        elif row[0] == \"U\":\n",
    "            pass\n",
    "\n",
    "        # Fetch SK_BrokerID and SK_CustomerID\n",
    "        #matching_broker_record = DimBroker[DimBroker['BrokerID'] == row[4]]\n",
    "        #matching_customer_record = DimCustomer[DimCustomer['CustomerID'] == row[5]]\n",
    "\n",
    "        SK_BrokerID = matching_broker_record['SK_BrokerID'].values[0]\n",
    "        SK_CustomerID = matching_customer_record['SK_CustomerID'].values[0]\n",
    "        \n",
    "        SK_BrokerID =0 #sprak.sql(\"select SK_BrokerID from DimBroker where BrokerID == '\"+str(row[4])+\"'\").collect()[0][0]\n",
    "        SK_CustomerID =0 #sprak.sql(\"select SK_CustomerID from DimCustomer where CustomerID == '\"+str(row[5])+\"'\").collect()[0][0]\n",
    "\n",
    "        Status = ''#sprak.sql(\"select ST_NAME from StatusType where ST_ID == '\"+str(row[6])+\"'\").collect()[0][0]\n",
    "        #StatusType[StatusType['ST_ID'] == row[6]]['ST_NAME'].values[0]\n",
    "\n",
    "\n",
    "        working_df_account = [serrogate_key_Account, row[1], SK_BrokerID, SK_CustomerID, Status, row[2], row[3], 1, batch_id, '', '9999-12-31']\n",
    "        #print(working_df_account)\n",
    "        #raise Exception(\"Sorry, no numbers below zero\") \n",
    "        csv_writer.writerow(working_df_account)\n",
    "        raise ValueError('A very specific bad thing happened.')\n",
    "        serrogate_key_Account += 1\n",
    "        \n",
    "    \n",
    "    sch = spark.sql(\"select * from DimAccount\").schema\n",
    "    df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_DimAccount.csv\", sep=',')   \n",
    "    #df.write.mode(\"append\").insertInto(\"DimAccount\")\n",
    "    \n",
    "\n",
    "def incr_DimCustomer(batch_id):\n",
    "    csv_file = open(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_DimCustomer.csv\", 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    customer_data = pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch\"+str(batch_id) +\"/Customer.txt\", header=None,sep=r'[,|]')\n",
    "    serrogate_key_Customer = spark.sql(\"select max(SK_CUSTOMERID) from DimCustomer\").collect()[0][0]\n",
    "    for index, row in customer_data.iterrows():\n",
    "        if row[0] == \"I\":\n",
    "            pass\n",
    "        elif row[0] == \"U\":\n",
    "            pass\n",
    "        if row[8] == 'F' or  row[8] == 'H':\n",
    "            Gender = row[8]\n",
    "        else: \n",
    "            Gender ='U'\n",
    "\n",
    "        matching_status_record = StatusType[StatusType['ST_ID'] == row[4]]\n",
    "        SK_StatusID = matching_status_record['SK_StatusID'].values[0]\n",
    "\n",
    "        Phone1 = str(row[18])+str(row[19])+str(row[20])+str(row[21])\n",
    "        Phone2 = str(row[22])+str(row[23])+str(row[24])+str(row[25])\n",
    "        Phone3 = str(row[26])+str(row[27])+str(row[28])+str(row[29])\n",
    "\n",
    "        NationalTax = spark.sql(\"select TX_NAME, TX_RATE from TaxRate where TX_ID ='\" +str(row[31]) + \"'\")\n",
    "        NationalTaxRateDesc = NationalTax.collect()[0][0]\n",
    "        NationalTaxRate = NationalTax.collect()[1][0]\n",
    "\n",
    "        LocalTax = spark.sql(\"select TX_NAME, TX_RATE from TaxRate where TX_ID = '\" + str(row[30]) + \"'\")\n",
    "        LocalTaxRateDesc = LocalTax.collect()[0][0]\n",
    "        LocalTaxRate = LocalTax.collect()[1][0]\n",
    "\n",
    "        working_df_customer = [\n",
    "            serrogate_key_Customer, row[1], row[2], Status, row[5], row[6], row[7], Gender, row[9], row[10],\n",
    "            row[11], row[12], row[13], row[14], row[15], Phone1,Phone2,Phone3,row[16],row[17], \n",
    "            NationalTaxRateDesc,NationalTaxRate,LocalTaxRateDesc,LocalTaxRate,\n",
    "            '','','','', 1, batch_id, '', '9999-12-31'\n",
    "        ]\n",
    "        csv_writer.writerow(working_df_customer)\n",
    "        serrogate_key_Customer += 1\n",
    "        \n",
    "        # VALERIO DIMESSAGES\n",
    "        if row[5] not in [1,2,3]:\n",
    "                df = [(current_timestamp(),batch_id,\"DimCustomer\",\"Invalid customer tier\",\"Alert\",f\"C_ID = {row[2]}, C_TIER ={row[5]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                \n",
    "            if row[6] < datetime(1923,11,15) or row[6] > (2023,11,15):\n",
    "                df = [(current_timestamp(),batch_id,\"DimCustomer\",\"DOB out of range\",\"Alert\",f\"C_ID = {row[2]}, C_DOB ={row[6]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "        # ---\n",
    "        \n",
    "    #inserting result\n",
    "    sch = spark.sql(\"select * from DimCustomer\").schema\n",
    "    df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_DimCustomer.csv\", sep=',')\n",
    "    #df.write.mode(\"append\").insertInto(\"DimCustomer\")\n",
    "    \n",
    "\n",
    "def inc_FactCashBalance(batch_id):\n",
    "    csv_file = open(os.getcwd() + \"/result/Batch\" + batch_id + \"_FactCashBalance.csv\", 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    cash_transaction_data = pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch\"+str(batch_id) +\"/CashTransaction.txt\", header=None,sep=r'[,|]')\n",
    "    \n",
    "    cash_transaction_data.columns = ['I/U', 'CT_ID', 'CT_CA_ID', 'CT_DATE', 'CT_AMT', 'CT_DESC']\n",
    "    Cash = 0 # ??\n",
    "    for index, row in cash_transaction_data.iterrows():\n",
    "        if row['I/U'] == \"I\": # TO-DO\n",
    "            pass\n",
    "        elif row['I/U'] == \"U\":\n",
    "            pass\n",
    "\n",
    "        \n",
    "        tmp = '' #spark.sql(\"select SK_CustomerID, SK_AccountID from DimAccount where AccountID = '\" \n",
    "            #       + row[2] + \"' and isCurrent == '1'\" )\n",
    "        SK_CustomerID ='' #tmp.collect()[0][0]\n",
    "        SK_AccountID ='' #tmp.collect()[0][1]\n",
    "        SK_DateID = ''#spark.sql(\"select SK_DateID from DimDate where Date == '\"+ row[2]+\"'\").collect()[0][0]\n",
    "        Cash = Cash + row[4]\n",
    "\n",
    "\n",
    "        working_df_factCash = [SK_CustomerID, SK_AccountID, SK_DateID, Cash, batch_id]\n",
    "        csv_writer.writerow(working_df_factCash)\n",
    "        \n",
    "    #inserting result\n",
    "    sch = spark.sql(\"select * from FactCashBalance\").schema\n",
    "    df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_FactCashBalance.csv\", sep=',')\n",
    "    #df.write.mode(\"append\").insertInto(\"FactCashBalance\")\n",
    "\n",
    "        \n",
    "def inc_DimTrade(batch_id): \n",
    "    \n",
    "    csv_file = open(os.getcwd() + \"/result/Batch\" + batch_id + \"_DimTrade.csv\", 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "    trade_data=  pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch\"+str(batch_id) +\"/Trade.txt\", header=None,sep=r'[,|]')\n",
    "    trade_data.columns = ['CDC_FLAG', 'T_ID', 'T_CA_ID', 'T_DTS', 'T_ST_ID', 'T_TT_ID', 'T_IS_CASH', 'T_S_SYMB', \n",
    "                          'T_QTY', 'T_BID_PRICE', 'T_CA_ID_B', 'T_CA_ID_S', 'T_TRADE_PRICE', 'T_CHRG', 'T_COMM', 'T_TAX']\n",
    "\n",
    "    for index, row in trade_data.iterrows():\n",
    "        print(row[3])\n",
    "        SK_CreateDateID = row[3].split(\" \")[0]\n",
    "        SK_CreateTimeID = row[3].split(\" \")[1]\n",
    "        SK_CloseDateID = None\n",
    "        SK_CloseTimeID = None\n",
    "        if row[4] in [\"CMPT\", \"CNCL\"]:\n",
    "            SK_CloseDateID = row[3].split(\" \")[0]\n",
    "            SK_CloseTimeID = row[3].split(\" \")[1]\n",
    "        tmp = '' #spark.sql(\"select SK_AccountID, SK_CustomerID, SK_BrokerID from DimAccount where AccountID = '\" \n",
    "                                  #       + row[?] + \"' and IsCurrent = '1'\"\n",
    "        SK_BrokerID = '' #tmp.collect()[0][0]\n",
    "        SK_CustomerID = '' #tmp.collect()[0][1]\n",
    "        SK_AccountID = '' #tmp.collect()[0][2]\n",
    "        Status = '' #spark.sql(\"select ST_NAME from StatusType where ST_ID = '\" + row[?]+ \"'\" )\n",
    "        Type = '' #spark.sql(\"select TT_NAME from TradeType where TT_ID = '\" + row[?]+ \"'\" )\n",
    "        tmp = '' #spark.sql(\"select SK_SecurityID, SK_CompanyID from DimSecurity where SYMBOL = '\" + row[1] + \n",
    "                           #          \"' and IsCurrent = '1'\")\n",
    "        SK_SecurityID = '' #tmp.collect()[0][0]\n",
    "        SK_CompanyID = '' #tmp.collect()[0][1]\n",
    "\n",
    "        working_df_trade = [row[1], SK_BrokerID, SK_CreateDateID, SK_CreateTimeID, SK_CloseDateID,\n",
    "                                SK_CloseTimeID, Status, Type, row[6], SK_SecurityID, SK_CompanyID,\n",
    "                                row[8], row[9], SK_CustomerID, SK_AccountID, row[11], row[12], row[13],\n",
    "                                row[14], row[15], batch_id]\n",
    "        csv_writer.writerow(working_df_trade)\n",
    "        \n",
    "        # DIMESSAGES VALERIO\n",
    "        if row[17] is not None and row[17] > (row[16] * row[11]):\n",
    "            df = [(current_timestamp(),batch_id,\"DimTrade\",\"Invalid trade fee\",\"Alert\",f\"T_ID = {row[0]}, T_CHRG ={row[17]}\")]\n",
    "            df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                \n",
    "        if row[18] is not None and row[18] > (row[16] * row[11]):\n",
    "            df = [(current_timestamp(),batch_id,\"DimTrade\",\"Invalid trade commission\",\"Alert\",f\"T_ID = {row[0]}, T_COMM ={row[18]}\")]\n",
    "            df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "        # ---\n",
    "        \n",
    "        \n",
    "    #inserting result\n",
    "    sch = spark.sql(\"select * from DimTrade\").schema\n",
    "    df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_DimTrade.csv\", sep=',')\n",
    "    #df.write.mode(\"append\").insertInto(\"DimTrade\")\n",
    "\n",
    "incr_DimAccount(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_FactHoldings(batch_id): \n",
    "    \n",
    "    csv_file = open(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_FactHoldings.csv\", 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "    data=  pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch\"+str(batch_id) +\"/HoldingHistory.txt\", header=None,sep=r'[,|]')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        TRADEID = row[2]\n",
    "        CURRENTTRADEID = row[3]\n",
    "        CURRENTHOLDING = row[5]\n",
    "        \n",
    "        df = spark.sql(\"\"\"select SK_CustomerID,SK_AccountID,SK_SECURITYID,SK_COMPANYID,\n",
    "                        SK_CLOSEDATEID,SK_CloseTimeID, SK_AccountID\n",
    "                               from DimTrade where TradeID = '\"\"\" + str(row[3]) + \"'\")\n",
    "        SK_CUSTOMERID =0 #df.collect()[0][0]\n",
    "        SK_ACCOUNTID =0 #df.collect()[0][1]\n",
    "        SK_SECURITYID =0 #df.collect()[0][2]\n",
    "        SK_COMPANYID =0 #df.collect()[0][3]\n",
    "        SK_DATEID =0 #df.collect()[0][4]\n",
    "        SK_TIMEID =0 #df.collect()[0][5]\n",
    "        CURRENTPRICE =0 #df.collect()[0][6]\n",
    "\n",
    "        working_df = [TRADEID, CURRENTTRADEID, SK_CUSTOMERID, SK_ACCOUNTID, SK_SECURITYID, SK_COMPANYID,\n",
    "                       SK_DATEID, SK_TIMEID, CURRENTPRICE, CURRENTHOLDING, batch_id]\n",
    "        csv_writer.writerow(working_df)\n",
    "\n",
    "    #inserting result\n",
    "    sch = spark.sql(\"select * from FactHoldings\").schema\n",
    "    df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_FactHoldings.csv\", sep=',')\n",
    "    #df.write.mode(\"append\").insertInto(\"FactHoldings\")\n",
    "    \n",
    "    \n",
    "def inc_FactMarketHistory(batch_id): \n",
    "    \n",
    "    csv_file = open(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_FactMarketHistory.csv\", 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "    data=  pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch\"+str(batch_id) +\"/DailyMarket.txt\", header=None,sep=r'[,|]')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        tmp = '' #spark.sql(\"select SK_CustomerID, SK_CompanyID from DimSecurity where Symbol = '\" + row[3] + \" where IsCurrent = '1'\")\n",
    "        SK_SECURITYID = '' #tmp.collect()[0][0]\n",
    "        SK_COMPANYID = '' #tmp.collect()[0][1]\n",
    "        SK_DATEID = '' #spark.sql(\"select SK_DateID from DimDate where DateValue = '\" + row[2] + \"'\")\n",
    "        PERATIO = ''\n",
    "        YIELD = '' #spark.sql(\"select dividend from DimSecurity where Symbol = '\" + row[3] + \" where IsCurrent = '1'\")/row[4]*100\n",
    "        \n",
    "        \n",
    "        FIFTYTWOWEEKHIGH = ''#spark.sql(\"select max(DM_HIGH) from FactMarketHistory \n",
    "                        #where\" + row[0] +\" <> date_sub(\"+row[0]+\", 365))\"\n",
    "        FIFTYTWOWEEKLOW= '' #spark.sql(\"select min(DM_LOW) from FactMarketHistory \n",
    "                        #where\" + row[0] +\" <> date_sub(\"+row[0]+\", 365))\"\n",
    "        SK_FIFTYTWOWEEKLOWDATE = '' # ??\n",
    "        SK_FIFTYTWOWEEKHIGHDATE = '' #??\n",
    "        CLOSEPRICE = row[4] \n",
    "        DAYHIGH = row[5]\n",
    "        DAYLOW = row[6]\n",
    "        VOLUME = row[7]\n",
    "\n",
    "        working_df = [SK_SECURITYID,SK_COMPANYID,SK_DATEID,PERATIO,YIELD,FIFTYTWOWEEKHIGH,\n",
    "                              SK_FIFTYTWOWEEKHIGHDATE,FIFTYTWOWEEKLOW,SK_FIFTYTWOWEEKLOWDATE,\n",
    "                              CLOSEPRICE,DAYHIGH,DAYLOW,VOLUME,batch_id]\n",
    "        csv_writer.writerow(working_df)\n",
    "     \n",
    "        if row[3] is None:\n",
    "            df = [(current_timestamp(),batch_id,\"FactMarketHistory\",\"No earnings for company\",\"Alert\",f\"DM_S_SYMB={row[3]}\")]\n",
    "            df.write.mode(\"append\").insertInto(\"DImessages\")   \n",
    "        \n",
    "        \n",
    "    #inserting result\n",
    "    sch = spark.sql(\"select * from FactMarketHistory\").schema\n",
    "    df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_FactMarketHistory.csv\", sep=',')\n",
    "    #df.write.mode(\"append\").insertInto(\"FactMarketHistory\")\n",
    "    \n",
    "def inc_FactWatches(batch_id): \n",
    "    \n",
    "    csv_file = open(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_FactWatches.csv\", 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "    data=  pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch\"+str(batch_id) +\"/WatchHistory.txt\", header=None,sep=r'[,|]')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        # SK_CUSTOMERID and SK_SECURITYID: slighly different (added \"IsCurrent=1\").\n",
    "        # Put in as a comment as this is how it was done in FactWatches's historical load\n",
    "        SK_CUSTOMERID = 0 #spark.sql(\"select SK_CustomerID from DimCustomer where CUSTOMERID = '\" + row[0] + \"'AND IsCurrent = 1\")\n",
    "        SK_SECURITYID = 0 #spark.sql(\"select SK_SecurityID from DimSecurity where SYMBOL = '\" + row[1] + \"'AND IsCurrent = 1\")\n",
    "        SK_DATEID_DATEPLACED=row[2]\n",
    "        if row[3]==\"ACTV\":\n",
    "            SK_DATEID_DATEREMOVED = ''\n",
    "        else:\n",
    "            SK_DATEID_DATEREMOVED = row[2]\n",
    "        \n",
    "\n",
    "        working_df = [SK_CUSTOMERID,SK_SECURITYID,SK_DATEID_DATEPLACED,SK_DATEID_DATEREMOVED,batch_id]\n",
    "        csv_writer.writerow(working_df)\n",
    "        \n",
    "    #inserting result\n",
    "    sch = spark.sql(\"select * from FactWatches\").schema\n",
    "    df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_FactWatches.csv\", sep=',')\n",
    "    #df.write.mode(\"append\").insertInto(\"FactWatches\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def inc_Prospect(batch_id):\n",
    "    \n",
    "    prospect_count = 0 \n",
    "    \n",
    "    # PERFORM UPDATE VALERIO\n",
    "    spark.sql(\"CREATE TABLE temp_Prospect AS SELECT * FROM Prospect\")\n",
    "    \n",
    "    csv_file = open(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_Prospect.csv\", 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "    data=  pd.read_csv(os.getcwd() + \"/data/SF\"+str(SF) +\"/Batch\"+str(batch_id) +\"/Prospect.csv\", header=None,sep=r'[,|]')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        \n",
    "        # DIMESSAGES VALERIO\n",
    "        prospect_count += 1\n",
    "        \n",
    "        isCustomer = spark.sql(\"select * from DimCustomer where status = 'ACTIVE' \")#to-do\n",
    "        if isCustomer.count() == 0:\n",
    "            isCustomer = 0\n",
    "        else:\n",
    "            isCustomer = 1\n",
    "        SK_RecordDateID = 0 #to-do (both historical and incremental)\n",
    "        SK_UpdateDateID = 0 #to-do (both historical and incremental)\n",
    "        \n",
    "        MarketingNameplate = \"\"\n",
    "        if row[12] > 200000 or row[21] > 1000000:\n",
    "            MarketingNameplate += \"+HighValue\"\n",
    "        if row[14] > 3 or row[20] > 5:\n",
    "            MarketingNameplate += \"+Expenses\"\n",
    "        if row[16] > 45:\n",
    "            MarketingNameplate += \"+Boomer\"\n",
    "        if row[12] < 100000 or row[21] < 50000 or row[17] < 600:\n",
    "            MarketingNameplate += \"+MoneyAlert\"\n",
    "        if row[13] > 3 or row[20] > 7 :\n",
    "            MarketingNameplate += \"+Spender\"\n",
    "        if row[16] < 25 and row[12] > 1000000 :\n",
    "            MarketingNameplate += \"+Inherited\"\n",
    "        if len(MarketingNameplate) != 0:\n",
    "            MarketingNameplate = MarketingNameplate[1:]\n",
    "        \n",
    "        \n",
    "        working_df = [row[0],SK_RecordDateID,SK_UpdateDateID,1,isCustomer\n",
    "                    ,row[1],row[2],row[3],row[4],row[5],row[6]\n",
    "                    ,row[7],row[8],row[9],row[10]\n",
    "                    ,row[11],row[12],row[13],row[14],row[15],row[16],\n",
    "                    row[17],row[18],row[19],row[20],row[21],MarketingNameplate]\n",
    "        csv_writer.writerow(working_df)\n",
    "    \n",
    "    # PERFORM UPDATE VALERIO\n",
    "    sch = spark.sql(\"select * from \" + i[1]).schema\n",
    "    df = spark.read.schema(sch).csv(os.getcwd() + \"/result/temp_\" + i[1] + \".csv\", sep=',')\n",
    "    if i[1] != \"Prospect\"\n",
    "        #df.write.mode(\"append\").insertInto(i[1])\n",
    "    else:\n",
    "        # Insert batch2's Prospect table values into the temporary table\n",
    "        df.write.mode(\"append\").insertInto(\"temp_Prospect\")\n",
    "        # Extract batch1's values not present in batch2\n",
    "        input_query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM Prospect as p\n",
    "        WHERE p.AgencyID NOT IN\n",
    "        (\n",
    "            SELECT tp.AgencyID\n",
    "            FROM temp_Prospect as tp\n",
    "        );\n",
    "        \"\"\"\n",
    "        # Append them to the temporary table, which now contains \"the updated\"\n",
    "        # values and the non-modified ones from Batch1\n",
    "        spark.sql(input_query).write.insertInto(\"temp_Prospect\", overwrite=False)\n",
    "        # Erase all Prospect rows and copy temp_Prospect into it\n",
    "        spark.sql(\"TRUNCATE TABLE Prospect\")\n",
    "        spark.sql(\"INSERT INTO Prospect SELECT * FROM temp_Prospect\")\n",
    "        # Erase all temp_Prospect rows\n",
    "        spark.sql(\"TRUNCATE TABLE temp_Prospect\")\n",
    "    \n",
    "    # DIMESSAGES VALERIO\n",
    "    df = [(current_timestamp(),batch_id,\"Prospect\",\"Source rows\",\"Status\",prospect_count)]\n",
    "    df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "    df = [(current_timestamp()batch_id,\"Prospect\",\"Inserted rows\",\"Status\",123)]\n",
    "    df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "    df = [(current_timestamp(),batch_id,\"Prospect\",\"Updated rows\",\"Status\",123)]\n",
    "    df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "        \n",
    "    # VERSION WITHOUT PERFORM UPDATE VALERIO\n",
    "    #inserting result\n",
    "    #sch = spark.sql(\"select * from Prospect\").schema\n",
    "    #df = spark.read.schema(sch).csv(os.getcwd() + \"/result/Batch\" + str(batch_id) + \"_Prospect.csv\", sep=',')\n",
    "    #df.write.mode(\"append\").insertInto(\"Prospect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIT VALERIO\n",
    "for i in range(2,4,1):\n",
    "    #incremental part\n",
    "    # VERIFY ORDER !!!!!!!\n",
    "    incr_DimAccount(i)\n",
    "    visibility_2_fun() \n",
    "    incr_DimCustomer(i)\n",
    "    visibility_2_fun()\n",
    "    inc_FactCashBalance(i)\n",
    "    visibility_2_fun()\n",
    "    inc_DimTrade(i)\n",
    "    visibility_2_fun()\n",
    "    inc_FactHoldings(i)\n",
    "    visibility_2_fun()\n",
    "    inc_FactMarketHistory(i)\n",
    "    visibility_2_fun()\n",
    "    inc_FactWatches(i)\n",
    "    visibility_2_fun()\n",
    "    inc_Prospect(i)\n",
    "    visibility_2_fun()\n",
    "    #test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIT VALERIO\n",
    "# Execute the following line during the Incremental Phase 1\n",
    "audit_upload(2)\n",
    "# At the end of the Incremental Phase 1: execute the batch validation query\n",
    "#spark.sql(os.getcwd() + \"/data/output/tpcdi_validation.sql\")\n",
    "\n",
    "f = open(os.getcwd() + \"/tpcdi_validation.sql\", \"r\")\n",
    "tables=f.read()\n",
    "tables = tables.split(\";\")[:-1]\n",
    "for i in tables:\n",
    "    spark.sql(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIT VALERIO\n",
    "# Execute the following line during the Incremental Phase 2\n",
    "audit_upload(3)\n",
    "# At the end of the Incremental Phase 2: execute the batch validation query\n",
    "#spark.sql(os.getcwd() + \"/data/output/tpcdi_validation.sql\")\n",
    "\n",
    "f = open(os.getcwd() + \"/tpcdi_validation.sql\", \"r\")\n",
    "tables=f.read()\n",
    "tables = tables.split(\";\")[:-1]\n",
    "for i in tables:\n",
    "    spark.sql(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Audit Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIT VALERIO\n",
    "# The following line execute the data visibility query 1. It has to be executed once at the start of the Automated Audit Phase\n",
    "#spark.sql(os.getcwd() + \"/data/output/tpcdi_visibility_1.sql\")\n",
    "\n",
    "f = open(os.getcwd() + \"/tpcdi_visibility_1.sql\", \"r\")\n",
    "tables=f.read()\n",
    "tables = tables.split(\";\")[:-1]\n",
    "for i in tables:\n",
    "    spark.sql(i)\n",
    "\n",
    "# Then the audit query has to be runned once\n",
    "#spark.sql(os.getcwd() + \"/data/output/tpcdi_audit.sql\")\n",
    "\n",
    "f = open(os.getcwd() + \"/tpcdi_audit.sql\", \"r\")\n",
    "tables=f.read()\n",
    "tables = tables.split(\";\")[:-1]\n",
    "for i in tables:\n",
    "    spark.sql(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIT VALERIO\n",
    "# At the start of the audit phase, erase \"temp_Prospect\"\n",
    "spark.sql(\"DROP TABLE IF EXISTS temp_Prospect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Automated Audit (metric computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In TPC-DI the times are given as output of the Batch Validation query,\n",
    "# so we need to retreive them from DImessages table\n",
    "\n",
    "# CT = Completion Timestamp\n",
    "CT0 = spark.sql(\n",
    "    \"select MessageDateAndTime from DImessages where BatchID = 0 and MessageType = ‘PCR’\"\n",
    ")\n",
    "CT1 = spark.sql(\n",
    "    \"select MessageDateAndTime from DImessages where BatchID = 1 and MessageType = ‘PCR’\"\n",
    ")\n",
    "CT2 = spark.sql(\n",
    "    \"select MessageDateAndTime from DImessages where BatchID = 2 and MessageType = ‘PCR’\"\n",
    ")\n",
    "CT3 = spark.sql(\n",
    "    \"select MessageDateAndTime from DImessages where BatchID = 3 and MessageType = ‘PCR’\"\n",
    ")\n",
    "\n",
    "# TH = Throughput of Historical Load\n",
    "# TH = RH / EH\n",
    "# --> RH = Row count of Batch1 (from \"digen_report.txt\")\n",
    "# --> EH = Elapsed time for Historical Load\n",
    "TH = 7804509 / (CT1 - CT0)\n",
    "\n",
    "# For incremental phases, similar to what did before, except that\n",
    "# the denominator is the maximum betweeen the elapsed time and 1800\n",
    "TI1 = 33380 / max(CT2 - CT1, 1800)\n",
    "TI2 = 33455 / max(CT3 - CT2, 1800)\n",
    "\n",
    "TPC_DI_RPS = round(gmean([TH, min(TI1, TI2)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
