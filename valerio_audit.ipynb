{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on how queries work\n",
    "\n",
    "### At the end of each Phase\n",
    "- Execute batch validation query\n",
    "- Write into DImessages a phase completion record\n",
    "\n",
    "### Time to time\n",
    "- Execution of Data Visibility Queries in historical and incremental\n",
    "\n",
    "### Automated Audit Phase\n",
    "(to be executed just after 2nd Incremental Update)\n",
    "\n",
    "1. Load audit data into the audit table (note that first row of each audit file is the header)\n",
    "2. Execute Data Visibility 1 query (appendix C)\n",
    "3. Execute the audit query (appendix A)\n",
    "\n",
    "Re-think of incremental insert then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO\n",
    "1. DONE: Load audit data 1 into the audit table\n",
    "2. DONE: Load audit data 2 into the audit table\n",
    "3. DONE: Load audit data 3 into the audit table\n",
    "4. DONE: Execute Data Visibility 1 query\n",
    "5. DONE: Execute audit query\n",
    "6. DONE: Batch validation in Historical and Incremental\n",
    "7. DONE: Data visibility queries time to time\n",
    "8. DONE: Metrics\n",
    "9. DONE: Re-think of incremental insert\n",
    "10. DImessages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visibility queries\n",
    "\n",
    "Those are a pain in the ass :)\n",
    "\n",
    "Basically from start of Incremental 1 up to the end of Incremental 2 we need to run **once** visibility 1 and then **n times** visibility 2. Between each execution no more than 5 minutes can elapse. So what I thought about?\n",
    "\n",
    "1. Run visibility 1 at the start of Incremental Phase 1 (so that surely less than 5 minutes elapse between start of Incremental 1 and Visibility 1)\n",
    "2. Run visibility 2 right after Visibility 2 (so that we're sure Visibility 2 is executed at least once)\n",
    "3. Start measuring time\n",
    "2. After each load, run \"visibility_2_fun\" function. It works as follows:\n",
    "    - Checks if more than 5 minutes elapsed (if so, return an error)\n",
    "    - Checks at least 2 minutes elapsed (we don't want to execute the visibility queries too frequently, as they may slow down the benchmark)\n",
    "    - If a time between 2 and 5 minutes elapsed, proceed:\n",
    "        - Run visibility 2\n",
    "        - Reset the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visibility_2_fun():\n",
    "    \n",
    "    global start_time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    if execution_time > 300: # 300 seconds = 5 minutes\n",
    "        print(\"ERROR: more than 5 minutes elapsed between visibility queries's executions\")\n",
    "        \n",
    "    elif execution_time < 300 and execution_time > 120:\n",
    "        spark.sql(os.getcwd() + \"/data/output/tpcdi_visibility_2.sql\")\n",
    "        start_time = time.time()\n",
    "        return\n",
    "        \n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Visibility 1 at the start of incremental\n",
    "spark.sql(os.getcwd() + \"/data/output/tpcdi_visibility_1.sql\")\n",
    "\n",
    "# Run Visibility 2 right after visibility 1\n",
    "spark.sql(os.getcwd() + \"/data/output/tpcdi_visibility_2.sql\")\n",
    "\n",
    "# Register time\n",
    "start_time = time.time()\n",
    "\n",
    "# Block of code...\n",
    "\n",
    "# After each block of code:\n",
    "visibility_2_fun()\n",
    "\n",
    "# Another block of code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Historical Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not done: create the audit table in tpcdi.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads audit data into the audit table. It can be used for all the three batches.\n",
    "\n",
    "def audit_upload(batch_n):\n",
    "    # Go to Batch1 and consider only the files that end for \"audit.csv\"\n",
    "    files_in_directory = os.listdir(os.getcwd() + f\"/data/output/{batch_n}/\")\n",
    "    audit_files = [file for file in files_in_directory if file.endswith(\"_audit.csv\")]\n",
    "\n",
    "    # For each audit_file...\n",
    "    for audit_file in audit_files:\n",
    "        # Place \"audit_file\" data into a Spark DF\n",
    "        file_path = os.path.join(os.getcwd() + f\"/data/output/{batch_n}/\", audit_file)\n",
    "        audit_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Append data to the Audit table\n",
    "        audit_df.write.insertInto(\"Audit\", overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just before startin the Historical Load: execute the batch validation query\n",
    "spark.sql(os.getcwd() + \"/data/output/tpcdi_validation.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the following line during the Historical Phase\n",
    "audit_upload(\"Batch1\")\n",
    "\n",
    "# ...\n",
    "\n",
    "# At the end of the Historical Load: execute the batch validation query\n",
    "spark.sql(os.getcwd() + \"/data/output/tpcdi_validation.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the following line during the Incremental Phase 1\n",
    "audit_upload(\"Batch2\")\n",
    "\n",
    "# ...\n",
    "\n",
    "# At the end of the Incremental Phase 1: execute the batch validation query\n",
    "spark.sql(os.getcwd() + \"/data/output/tpcdi_validation.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the following line during the Incremental Phase 2\n",
    "audit_upload(\"Batch3\")\n",
    "\n",
    "# ...\n",
    "\n",
    "# At the end of the Incremental Phase 2: execute the batch validation query\n",
    "spark.sql(os.getcwd() + \"/data/output/tpcdi_validation.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Audit Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The followinf line execute the data visibility query 1. It has to be executed once at the start of the Automated Audit Phase\n",
    "spark.sql(os.getcwd() + \"/data/output/tpcdi_visibility_1.sql\")\n",
    "\n",
    "# Then the audit query has to be runned once\n",
    "spark.sql(os.getcwd() + \"/data/output/tpcdi_audit.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Automated Audit (metric computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In TPC-DI the times are given as output of the Batch Validation query,\n",
    "# so we need to retreive them from DImessages table\n",
    "\n",
    "# CT = Completion Timestamp\n",
    "CT0 = spark.sql(\n",
    "    \"select MessageDateAndTime from DImessages where BatchID = 0 and MessageType = ‘PCR’\"\n",
    ")\n",
    "CT1 = spark.sql(\n",
    "    \"select MessageDateAndTime from DImessages where BatchID = 1 and MessageType = ‘PCR’\"\n",
    ")\n",
    "CT2 = spark.sql(\n",
    "    \"select MessageDateAndTime from DImessages where BatchID = 2 and MessageType = ‘PCR’\"\n",
    ")\n",
    "CT3 = spark.sql(\n",
    "    \"select MessageDateAndTime from DImessages where BatchID = 3 and MessageType = ‘PCR’\"\n",
    ")\n",
    "\n",
    "# TH = Throughput of Historical Load\n",
    "# TH = RH / EH\n",
    "# --> RH = Row count of Batch1 (from \"digen_report.txt\")\n",
    "# --> EH = Elapsed time for Historical Load\n",
    "TH = 7804509 / (CT1 - CT0)\n",
    "\n",
    "# For incremental phases, similar to what did before, except that\n",
    "# the denominator is the maximum betweeen the elapsed time and 1800\n",
    "TI1 = 33380 / max(CT2 - CT1, 1800)\n",
    "TI2 = 33455 / max(CT3 - CT2, 1800)\n",
    "\n",
    "TPC_DI_RPS = round(gmean([TH, min(TI1, TI2)]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
