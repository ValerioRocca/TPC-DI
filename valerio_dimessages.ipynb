{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT !!!\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the Historical Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratings = [\"AAA\", \"AA[+/-]\", \"A[+/-]\", \"BBB[+/-]\", \"BB[+/-]\", \"B[+/-]\", \"CCC[+/-]\", \"CC\", \"C\", \"D\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DimCompany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if finwire_df_CMP.empty  == False:\n",
    "        finwire_df_CMP.to_csv(\"CMP\", index=False)\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "            Industry = spark.sql(\"select IN_NAME from INDUSTRY where IN_ID = '\" + row[5] + \"'\").collect()[0][0]\n",
    "            Status = spark.sql(\"select ST_NAME from StatusType where ST_ID = '\" + row[4] + \"'\").collect()[0][0]\n",
    "            \n",
    "            if row[6] not in valid_ratings:\n",
    "                df = [(current_timestamp(),1,\"DimCompany\",\"Invalid SPRating\",\"Alert\",f\"CO_ID = {row[3]}, CO_SP_RATE ={row[6]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                isLowGrade = None\n",
    "                row[6] = None\n",
    "                \n",
    "            else:                \n",
    "                if row[6].startswith('A') or row[6].startswith('BBB'):\n",
    "                    isLowGrade = 0\n",
    "                else:\n",
    "                    isLowGrade = 1\n",
    "            \n",
    "            #reorder\n",
    "            working_df = [serrogate_key_CMP,row[3],Status,row[2],Industry,row[6],isLowGrade,row[14],row[8],row[9]\n",
    "                          ,row[10],row[11],row[12],row[13],row[15],row[7],1,1\n",
    "                          ,string_to_timestamp(row[0]),'9999-12-31']\n",
    "            csv_writer.writerow(working_df)\n",
    "            \n",
    "            # ...\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DimCustomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif i[1] == \"DimAccount0\":\n",
    "        csv_file = open(os.getcwd() + \"/result/\" + i[1] + \".csv\", 'w', newline='')\n",
    "\n",
    "            # ...\n",
    "            \n",
    "            serrogate_key_Account += 1\n",
    "            \n",
    "            if row[5] not in [1,2,3]:\n",
    "                df = [(current_timestamp(),1,\"DimCustomer\",\"Invalid customer tier\",\"Alert\",f\"C_ID = {row[2]}, C_TIER ={row[5]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                \n",
    "            if row[6] < datetime(1923,11,15) or row[6] > (2023,11,15):\n",
    "                df = [(current_timestamp(),1,\"DimCustomer\",\"DOB out of range\",\"Alert\",f\"C_ID = {row[2]}, C_DOB ={row[6]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DimTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif i[1] == \"DimTrade\":\n",
    "        csv_file = open(os.getcwd() + \"/result/\" + i[1] + \".csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # ...\n",
    "        \n",
    "        for index, row in trade_data.iterrows():\n",
    "            time = spark.sql(\"select SK_DATEID from DimTime where SECONDDESC == '\"+row[1].split(\" \")[1] + \"'\").collect()[0][0]\n",
    "            \n",
    "            # ...\n",
    "            \n",
    "            csv_writer.writerow(working_df_trade)\n",
    "            \n",
    "            if row[17] is not None and row[17] > (row[16] * row[11]):\n",
    "                df = [(current_timestamp(),1,\"DimTrade\",\"Invalid trade fee\",\"Alert\",f\"T_ID = {row[0]}, T_CHRG ={row[17]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                \n",
    "            if row[18] is not None and row[18] > (row[16] * row[11]):\n",
    "                df = [(current_timestamp(),1,\"DimTrade\",\"Invalid trade commission\",\"Alert\",f\"T_ID = {row[0]}, T_COMM ={row[18]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FactMarketHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif i[1] == \"FactMarketHistory\":\n",
    "                \n",
    "                # ...\n",
    "\n",
    "                working_df = [SK_SecurityID,SK_COMPANYID,SK_DATEID,PERATIO,YIELD,FIFTYTWOWEEKHIGH,\n",
    "                              SK_FIFTYTWOWEEKHIGHDATE,FIFTYTWOWEEKLOW,SK_FIFTYTWOWEEKLOWDATE,\n",
    "                              CLOSEPRICE,DAYHIGH,DAYLOW,VOLUME,BATCHID]\n",
    "                \n",
    "                if row[3] is None:\n",
    "                     df = [(current_timestamp(),1,\"FactMarketHistory\",\"No earnings for company\",\"Alert\",f\"DM_S_SYMB={row[3]}\")]\n",
    "                     df.write.mode(\"append\").insertInto(\"DImessages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prospect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_count = 0\n",
    "\n",
    "# For each couple of [input,output] table\n",
    "for i in Modified_table_with_dependence:\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    if i[1] == \"FactHoldings\" or False:\n",
    "        # Open or create the output\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        # For each row, create a new row based on the output file\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            # ...\n",
    "\n",
    "        elif i[1] == \"Prospect\":\n",
    "            \n",
    "            prospect_count += 1\n",
    "            \n",
    "            # ...\n",
    "\n",
    "            # Input that row\n",
    "            csv_writer.writerow(working_df)\n",
    "\n",
    "        #add result \n",
    "        sch = spark.sql(\"select * from \" + i[1]).schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/\" + i[1] + \".csv\", sep=',')\n",
    "        #df.write.mode(\"append\").insertInto(i[1])\n",
    "\n",
    "        if i[1] == \"Prospect\":\n",
    "            df = [(current_timestamp(),1,\"Prospect\",\"Inserted rows\",\"Status\",prospect_count)]\n",
    "            df.write.mode(\"append\").insertInto(\"DImessages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MessageDateAndTime, BatchID, MessageSource, MessageText, MessageType, MessageData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DimCustomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif i[1] == \"DimAccount0\":\n",
    "        csv_file = open(os.getcwd() + \"/result/\" + i[1] + \".csv\", 'w', newline='')\n",
    "\n",
    "            # ...\n",
    "            \n",
    "            serrogate_key_Account += 1\n",
    "            \n",
    "            if row[5] not in [1,2,3]:\n",
    "                df = [(current_timestamp(),2,\"DimCustomer\",\"Invalid customer tier\",\"Alert\",f\"C_ID = {row[2]}, C_TIER ={row[5]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                \n",
    "            if row[6] < datetime(1923,11,15) or row[6] > (2023,11,15):\n",
    "                df = [(current_timestamp(),2,\"DimCustomer\",\"DOB out of range\",\"Alert\",f\"C_ID = {row[2]}, C_DOB ={row[6]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DimTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif i[1] == \"DimTrade\":\n",
    "        csv_file = open(os.getcwd() + \"/result/\" + i[1] + \".csv\", 'w', newline='')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # ...\n",
    "        \n",
    "        for index, row in trade_data.iterrows():\n",
    "            time = spark.sql(\"select SK_DATEID from DimTime where SECONDDESC == '\"+row[1].split(\" \")[1] + \"'\").collect()[0][0]\n",
    "            \n",
    "            # ...\n",
    "            \n",
    "            csv_writer.writerow(working_df_trade)\n",
    "            \n",
    "            if row[17] is not None and row[17] > (row[16] * row[11]):\n",
    "                df = [(current_timestamp(),2,\"DimTrade\",\"Invalid trade fee\",\"Alert\",f\"T_ID = {row[0]}, T_CHRG ={row[17]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "                \n",
    "            if row[18] is not None and row[18] > (row[16] * row[11]):\n",
    "                df = [(current_timestamp(),2,\"DimTrade\",\"Invalid trade commission\",\"Alert\",f\"T_ID = {row[0]}, T_COMM ={row[18]}\")]\n",
    "                df.write.mode(\"append\").insertInto(\"DImessages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FactMarketHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif i[1] == \"FactMarketHistory\":\n",
    "                \n",
    "                # ...\n",
    "\n",
    "                working_df = [SK_SecurityID,SK_COMPANYID,SK_DATEID,PERATIO,YIELD,FIFTYTWOWEEKHIGH,\n",
    "                              SK_FIFTYTWOWEEKHIGHDATE,FIFTYTWOWEEKLOW,SK_FIFTYTWOWEEKLOWDATE,\n",
    "                              CLOSEPRICE,DAYHIGH,DAYLOW,VOLUME,BATCHID]\n",
    "                \n",
    "                if row[3] is None:\n",
    "                     df = [(current_timestamp(),2,\"FactMarketHistory\",\"No earnings for company\",\"Alert\",f\"DM_S_SYMB={row[3]}\")]\n",
    "                     df.write.mode(\"append\").insertInto(\"DImessages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prospect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect_count = 0\n",
    "\n",
    "# For each couple of [input,output] table\n",
    "for i in Modified_table_with_dependence:\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    if i[1] == \"FactHoldings\" or False:\n",
    "        # Open or create the output\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        # For each row, create a new row based on the output file\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            # ...\n",
    "\n",
    "        elif i[1] == \"Prospect\":\n",
    "            \n",
    "            prospect_count += 1\n",
    "            \n",
    "            # ...\n",
    "\n",
    "            # Input that row\n",
    "            csv_writer.writerow(working_df)\n",
    "\n",
    "        #add result \n",
    "        sch = spark.sql(\"select * from \" + i[1]).schema\n",
    "        df = spark.read.schema(sch).csv(os.getcwd() + \"/result/\" + i[1] + \".csv\", sep=',')\n",
    "        #df.write.mode(\"append\").insertIntoSS(i[1])\n",
    "\n",
    "        if i[1] == \"Prospect\":\n",
    "            df = [(current_timestamp(),2,\"Prospect\",\"Source rows\",\"Status\",prospect_count)]\n",
    "            df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "            df = [(current_timestamp(),2,\"Prospect\",\"Inserted rows\",\"Status\",123)]\n",
    "            df.write.mode(\"append\").insertInto(\"DImessages\")\n",
    "            df = [(current_timestamp(),2,\"Prospect\",\"Updated rows\",\"Status\",123)]\n",
    "            df.write.mode(\"append\").insertInto(\"DImessages\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
